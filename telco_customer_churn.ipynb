{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMl2AgRATtfzCvMBSWPUqlo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshatha7710/telco-customer-churn/blob/main/telco_customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Full Upgraded Coursework Script (Optimized for High Grade)\n",
        "# ---------------------------\n",
        "\n",
        "# Core imports\n",
        "import os, json\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, confusion_matrix\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Optional libraries\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except Exception:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "# Random state\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Create outputs folder\n",
        "os.makedirs(\"outputs\", exist_ok=True)\n",
        "\n",
        "# ---------------------------\n",
        "# Load Dataset\n",
        "# ---------------------------\n",
        "data_path = \"data/Telco-Customer-Churn.csv\"\n",
        "if not os.path.exists(data_path):\n",
        "    print(f\"[WARN] Dataset not found at {data_path}, creating synthetic data\")\n",
        "    from sklearn.datasets import make_classification\n",
        "    X_synthetic, y_synthetic = make_classification(\n",
        "        n_samples=2000, n_features=20, n_informative=10,\n",
        "        n_redundant=5, n_classes=2, weights=[0.69, 0.31],\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "    df = pd.DataFrame(X_synthetic, columns=[f\"feature_{i}\" for i in range(20)])\n",
        "    df[\"Churn\"] = np.where(y_synthetic==1, \"Yes\", \"No\")\n",
        "else:\n",
        "    df = pd.read_csv(data_path)\n",
        "\n",
        "print(f\"[INFO] Dataset loaded. Shape: {df.shape}\")\n",
        "\n",
        "# ---------------------------\n",
        "# Ensure unique CustomerID\n",
        "# ---------------------------\n",
        "if 'customerID' not in df.columns:\n",
        "    df.insert(0, 'customerID', ['CUST' + str(i).zfill(5) for i in range(1, len(df)+1)])\n",
        "else:\n",
        "    # Ensure no duplicates\n",
        "    if df['customerID'].duplicated().any():\n",
        "        df['customerID'] = ['CUST' + str(i).zfill(5) for i in range(1, len(df)+1)]\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# EDA (Task 1)\n",
        "# ---------------------------\n",
        "print(\"----- EDA -----\")\n",
        "\n",
        "if 'TotalCharges' in df.columns:\n",
        "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "# Show a sample that includes customerID for the report appendix, then drop ID before model prep\n",
        "print(df.head())   # this prints the sample with customerID present (if it exists)\n",
        "if 'customerID' in df.columns:\n",
        "    # drop before preprocessing (customerID is an identifier, not a feature)\n",
        "    df.drop(columns=['customerID'], inplace=True)\n",
        "\n",
        "\n",
        "# 1. Basic checks\n",
        "print(df.head())\n",
        "print(\"\\nMissing values:\\n\", df.isnull().sum())\n",
        "print(\"\\nTarget distribution:\\n\", df[\"Churn\"].value_counts())\n",
        "\n",
        "# 2. Target distribution\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='Churn', data=df)\n",
        "plt.title('Churn Distribution')\n",
        "plt.savefig(\"outputs/churn_distribution.png\")\n",
        "plt.close()\n",
        "\n",
        "# 3. Numeric features EDA\n",
        "numerical_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "for col in numerical_cols:\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.histplot(df[col], bins=30, kde=True)\n",
        "    plt.title(f'{col} Histogram')\n",
        "    plt.savefig(f\"outputs/{col}_hist.png\")\n",
        "    plt.close()\n",
        "\n",
        "    # Boxplot for outlier detection\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.boxplot(x='Churn', y=col, data=df)\n",
        "    plt.title(f'{col} Boxplot by Churn')\n",
        "    plt.savefig(f\"outputs/{col}_boxplot.png\")\n",
        "    plt.close()\n",
        "\n",
        "# 4. Correlation heatmap\n",
        "plt.figure(figsize=(12,10))\n",
        "sns.heatmap(df[numerical_cols].corr(), annot=True, fmt=\".2f\", cmap='coolwarm')\n",
        "plt.title(\"Correlation Heatmap of Numeric Features\")\n",
        "plt.savefig(\"outputs/correlation_heatmap.png\")\n",
        "plt.close()\n",
        "\n",
        "print(\"[INFO] EDA plots saved in outputs/\")\n",
        "\n",
        "# ---------------------------\n",
        "# Preprocessing (Task 2)\n",
        "# ---------------------------\n",
        "def preprocess(df):\n",
        "    categorical_cols = df.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "    if \"Churn\" in categorical_cols:\n",
        "        categorical_cols.remove(\"Churn\")\n",
        "\n",
        "    numerical_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "\n",
        "    cat_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    num_pipeline = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        ('num', num_pipeline, numerical_cols),\n",
        "        ('cat', cat_pipeline, categorical_cols)\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "    X = df.drop(columns=\"Churn\")\n",
        "    y = df[\"Churn\"].map({\"No\":0, \"Yes\":1})\n",
        "\n",
        "    X_trans = preprocessor.fit_transform(X)\n",
        "\n",
        "    # Handle imbalance\n",
        "    if IMBLEARN_AVAILABLE:\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_trans, y = sm.fit_resample(X_trans, y)\n",
        "\n",
        "    return X_trans, y, preprocessor, categorical_cols, numerical_cols\n",
        "\n",
        "X, y, preprocessor, cat_cols, num_cols = preprocess(df)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# ---------------------------\n",
        "# Models & Hyperparameter Tuning\n",
        "# ---------------------------\n",
        "models = {\n",
        "    \"dummy\": DummyClassifier(strategy=\"most_frequent\"),\n",
        "    \"random_forest\": RandomForestClassifier(random_state=RANDOM_STATE, n_estimators=100),\n",
        "}\n",
        "\n",
        "if XGBOOST_AVAILABLE:\n",
        "    models[\"xgboost\"] = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE)\n",
        "\n",
        "# Tuned Decision Tree\n",
        "dt_model = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
        "dt_param_grid = {'max_depth':[5,10,15], 'min_samples_leaf':[5,10]}\n",
        "tuned_dt = GridSearchCV(dt_model, dt_param_grid, cv=3, scoring='f1', n_jobs=1)\n",
        "models[\"tuned_decision_tree\"] = tuned_dt\n",
        "\n",
        "# Tuned Neural Network\n",
        "nn_model = MLPClassifier(max_iter=300, random_state=RANDOM_STATE, early_stopping=True)\n",
        "nn_param_grid = {'hidden_layer_sizes':[(32,16),(64,32),(64,)], 'alpha':[0.0001,0.001]}\n",
        "tuned_nn = GridSearchCV(nn_model, nn_param_grid, cv=3, scoring='f1', n_jobs=1)\n",
        "models[\"tuned_neural_network\"] = tuned_nn\n",
        "\n",
        "# ---------------------------\n",
        "# Training & Evaluation\n",
        "# ---------------------------\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"[INFO] Training {name}\")\n",
        "    model.fit(X_train, y_train)\n",
        "    best_model = model.best_estimator_ if hasattr(model,'best_estimator_') else model\n",
        "\n",
        "    y_pred = best_model.predict(X_test)\n",
        "    y_proba = best_model.predict_proba(X_test)[:,1] if hasattr(best_model,\"predict_proba\") else y_pred\n",
        "\n",
        "    results[name] = {\n",
        "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "        \"precision\": precision_score(y_test, y_pred, zero_division=0),\n",
        "        \"recall\": recall_score(y_test, y_pred, zero_division=0),\n",
        "        \"f1\": f1_score(y_test, y_pred, zero_division=0),\n",
        "        \"roc_auc\": roc_auc_score(y_test, y_proba),\n",
        "        \"pr_auc\": average_precision_score(y_test, y_proba),\n",
        "        \"confusion_matrix\": confusion_matrix(y_test, y_pred).tolist()\n",
        "    }\n",
        "\n",
        "    if hasattr(model,'best_params_'):\n",
        "        results[name][\"best_params\"] = model.best_params_\n",
        "        print(f\"       Best Parameters: {model.best_params_}\")\n",
        "\n",
        "    # Confusion matrix plot\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(5,4))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(f\"{name} Confusion Matrix\")\n",
        "    plt.savefig(f\"outputs/{name}_confusion_matrix.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Save results\n",
        "with open(\"outputs/results.json\",\"w\") as f:\n",
        "    json.dump(results,f,indent=4)\n",
        "\n",
        "print(\"[INFO] Training complete. Results saved to outputs/results.json\")\n",
        "\n",
        "# ---------------------------\n",
        "# Feature Importance & Model Plots\n",
        "# ---------------------------\n",
        "try:\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "except AttributeError:\n",
        "    feature_names = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
        "\n",
        "# Feature importance for tree models\n",
        "for name in [\"tuned_decision_tree\",\"random_forest\",\"xgboost\"]:\n",
        "    if name in models:\n",
        "        model_to_check = models[name].best_estimator_ if hasattr(models[name],'best_estimator_') else models[name]\n",
        "        if hasattr(model_to_check,\"feature_importances_\"):\n",
        "            fi = model_to_check.feature_importances_\n",
        "            sorted_idx = fi.argsort()[-20:]\n",
        "            plt.figure(figsize=(12,8))\n",
        "            plt.barh(feature_names[sorted_idx], fi[sorted_idx])\n",
        "            plt.title(f\"{name} Feature Importances\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"outputs/{name}_feature_importance.png\")\n",
        "            plt.close()\n",
        "\n",
        "# MLP loss curve\n",
        "if \"tuned_neural_network\" in models:\n",
        "    nn_model = models[\"tuned_neural_network\"].best_estimator_\n",
        "    if hasattr(nn_model,\"loss_curve_\"):\n",
        "        plt.figure(figsize=(8,5))\n",
        "        plt.plot(nn_model.loss_curve_)\n",
        "        plt.title(\"Tuned Neural Network Training Loss Curve\")\n",
        "        plt.xlabel(\"Iteration\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid(True)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(\"outputs/nn_loss_curve.png\")\n",
        "        plt.close()\n",
        "\n",
        "# ---------------------------\n",
        "# SHAP Explanations (Ethics Discussion)\n",
        "# ---------------------------\n",
        "if SHAP_AVAILABLE and XGBOOST_AVAILABLE:\n",
        "    print(\"[INFO] Generating SHAP explanations\")\n",
        "    xgb_model = models[\"xgboost\"].best_estimator_ if hasattr(models[\"xgboost\"],'best_estimator_') else models[\"xgboost\"]\n",
        "    if hasattr(xgb_model,'feature_importances_'):\n",
        "        explainer = shap.TreeExplainer(xgb_model)\n",
        "        X_sample = X_test[:200]\n",
        "        feature_names_list = feature_names.tolist()\n",
        "        try:\n",
        "            shap_values = explainer.shap_values(X_sample)\n",
        "            X_sample_df = pd.DataFrame(X_sample, columns=feature_names_list)\n",
        "            shap.summary_plot(shap_values, X_sample_df, show=False)\n",
        "            plt.savefig(\"outputs/shap_summary.png\")\n",
        "            plt.close()\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] SHAP plotting failed: {e}\")\n",
        "\n",
        "print(\"[INFO] All outputs saved in: outputs/\")\n",
        "print(\"[INFO] AI Ethics: SMOTE used to address class imbalance, SHAP for explainability.\")\n",
        "print(\"[INFO] Post-deployment: Monitor model drift, retrain periodically, log misclassifications for fairness analysis.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPIqjtFnVN-m",
        "outputId": "554dc4cc-4faa-435f-bfde-eb12586a5920"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] Dataset not found at data/Telco-Customer-Churn.csv, creating synthetic data\n",
            "[INFO] Dataset loaded. Shape: (2000, 21)\n",
            "----- EDA -----\n",
            "  customerID  feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
            "0  CUST00001   0.836084  -0.406200   0.985852  -1.098696   0.896335   \n",
            "1  CUST00002  -0.540182  -5.246919  -0.296145  -1.540667  -0.455911   \n",
            "2  CUST00003   0.958855  -0.294964  -0.599162  -2.020151   0.696867   \n",
            "3  CUST00004   2.880514   0.903751   0.953711  -1.972112  -1.039654   \n",
            "4  CUST00005  -1.945330  -4.153046  -0.808245   0.836231  -1.757355   \n",
            "\n",
            "   feature_5  feature_6  feature_7  feature_8  ...  feature_11  feature_12  \\\n",
            "0   0.755028  -1.412918  -0.556318   0.570253  ...   -0.897290   -0.326756   \n",
            "1  -0.262313  -0.244954   2.069706  -2.330663  ...    2.734267    3.698911   \n",
            "2   2.054202   1.264476  -0.387519  -0.821913  ...    0.722353    1.012499   \n",
            "3  -0.190190   0.385689  -1.639310   0.116479  ...   -2.197232    2.682806   \n",
            "4  -0.974836  -1.006793  -2.172324   0.476773  ...    2.543219    3.475602   \n",
            "\n",
            "   feature_13  feature_14  feature_15  feature_16  feature_17  feature_18  \\\n",
            "0    2.159997   -0.100288   -1.362565    2.363513   -1.665928    1.098537   \n",
            "1    3.031550    3.538711   -4.532127   -1.520400    0.991898    1.359931   \n",
            "2   -0.078173    0.586247   -1.732252    0.228819    0.981800    0.529595   \n",
            "3   -0.727539   -2.156471    2.806395    4.401949    0.013505    2.266309   \n",
            "4   -0.420226    1.008474   -0.171578   -2.436469    1.702363    1.255665   \n",
            "\n",
            "   feature_19  Churn  \n",
            "0   -1.427495     No  \n",
            "1   -3.876972    Yes  \n",
            "2   -1.357666     No  \n",
            "3   -0.412491    Yes  \n",
            "4   -3.534173     No  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Missing values:\n",
            " customerID    0\n",
            "feature_0     0\n",
            "feature_1     0\n",
            "feature_2     0\n",
            "feature_3     0\n",
            "feature_4     0\n",
            "feature_5     0\n",
            "feature_6     0\n",
            "feature_7     0\n",
            "feature_8     0\n",
            "feature_9     0\n",
            "feature_10    0\n",
            "feature_11    0\n",
            "feature_12    0\n",
            "feature_13    0\n",
            "feature_14    0\n",
            "feature_15    0\n",
            "feature_16    0\n",
            "feature_17    0\n",
            "feature_18    0\n",
            "feature_19    0\n",
            "Churn         0\n",
            "dtype: int64\n",
            "\n",
            "Target distribution:\n",
            " Churn\n",
            "No     1377\n",
            "Yes     623\n",
            "Name: count, dtype: int64\n",
            "[INFO] EDA plots saved in outputs/\n",
            "[INFO] Training dummy\n",
            "[INFO] Training random_forest\n",
            "[INFO] Training xgboost\n",
            "[INFO] Training tuned_decision_tree\n",
            "       Best Parameters: {'max_depth': 5, 'min_samples_leaf': 5}\n",
            "[INFO] Training tuned_neural_network\n",
            "       Best Parameters: {'alpha': 0.001, 'hidden_layer_sizes': (32, 16)}\n",
            "[INFO] Training complete. Results saved to outputs/results.json\n",
            "[INFO] Generating SHAP explanations\n",
            "[WARN] SHAP plotting failed: Shape of passed values is (200, 1), indices imply (200, 2020)\n",
            "[INFO] All outputs saved in: outputs/\n",
            "[INFO] AI Ethics: SMOTE used to address class imbalance, SHAP for explainability.\n",
            "[INFO] Post-deployment: Monitor model drift, retrain periodically, log misclassifications for fairness analysis.\n"
          ]
        }
      ]
    }
  ]
}