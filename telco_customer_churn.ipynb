{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvSNzzMwHkglLo29C35kok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshatha7710/telco-customer-churn/blob/main/telco_customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZh7DS0FkwvL"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Telco Customer Churn - Research-grade end-to-end script (Version A)\n",
        "\n",
        "Features included:\n",
        "- Load real Telco Customer Churn CSV (or create synthetic fallback)\n",
        "- Full EDA (statistics + plots saved to `outputs/figures/`)\n",
        "- Advanced preprocessing (imputation, encoding, scaling)\n",
        "- Feature engineering and selection (mutual info + correlation filter)\n",
        "- Class imbalance handling (SMOTE)\n",
        "- Multiple models:\n",
        "    - Logistic Regression (baseline)\n",
        "    - Decision Tree\n",
        "    - Random Forest\n",
        "    - Gradient Boosting (XGBoost if available, else sklearn's HistGradientBoosting)\n",
        "    - Neural Network (Keras)\n",
        "- Hyperparameter tuning (RandomizedSearchCV for heavier models)\n",
        "- Cross-validation and repeated experiments\n",
        "- Model evaluation: accuracy, precision, recall, f1, ROC-AUC, PR-AUC, confusion matrices\n",
        "- Model explainability:\n",
        "    - SHAP explanations (TreeExplainer for tree-based models, KernelExplainer fallback)\n",
        "    - LIME example (if lime installed)\n",
        "- Real-time inference function (pipeline)\n",
        "- Post-deployment considerations: drift detection via PSI and periodic evaluation\n",
        "- Anomaly scoring via IsolationForest (fraud-like detection)\n",
        "- Save models, preprocessors, metrics, and artifacts to outputs/\n",
        "- Safe optional imports (xgboost, lightgbm, shap, lime) handled gracefully\n",
        "\n",
        "Usage:\n",
        "- Place `Telco-Customer-Churn.csv` in the `data/` directory (script will create synthetic dataset otherwise).\n",
        "- Run: python telco_churn_research.py\n",
        "- Outputs go into `outputs/` directory.\n",
        "\n",
        "Author: Generated for coursework\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import joblib\n",
        "import math\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score,\n",
        "    StratifiedKFold\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    average_precision_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Optional advanced models\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except Exception:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "\n",
        "# Neural network\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    KERAS_AVAILABLE = True\n",
        "except Exception:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "# Explainability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from lime import lime_tabular\n",
        "    LIME_AVAILABLE = True\n",
        "except Exception:\n",
        "    LIME_AVAILABLE = False\n",
        "\n",
        "# Imbalance handling\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except Exception:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------------------\n",
        "# Configuration & directories\n",
        "# ---------------------------\n",
        "ROOT = Path.cwd()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "OUT_DIR = ROOT / \"outputs\"\n",
        "FIG_DIR = OUT_DIR / \"figures\"\n",
        "MODEL_DIR = OUT_DIR / \"models\"\n",
        "REPORTS_DIR = OUT_DIR / \"reports\"\n",
        "\n",
        "for d in (DATA_DIR, OUT_DIR, FIG_DIR, MODEL_DIR, REPORTS_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_FILE = DATA_DIR / \"Telco-Customer-Churn.csv\"\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# ---------------------------\n",
        "# Utility Functions\n",
        "# ---------------------------\n",
        "def save_json(obj: Any, path: Path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def ensure_df_has_target(df: pd.DataFrame, target: str = \"Churn\") -> pd.DataFrame:\n",
        "    if target not in df.columns:\n",
        "        raise ValueError(f\"target column '{target}' not in dataframe\")\n",
        "    return df\n",
        "\n",
        "def safe_head(df: pd.DataFrame, n=5):\n",
        "    print(df.head(n).to_string())\n",
        "\n",
        "# ---------------------------\n",
        "# Data Loading / Synthetic\n",
        "# ---------------------------\n",
        "def create_synthetic_telco(path: Path, n=2000) -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(RANDOM_STATE)\n",
        "    df = pd.DataFrame({\n",
        "        \"customerID\": [f\"CUST{100000+i}\" for i in range(n)],\n",
        "        \"gender\": rng.choice([\"Female\", \"Male\"], n),\n",
        "        \"SeniorCitizen\": rng.choice([0,1], n, p=[0.85,0.15]),\n",
        "        \"Partner\": rng.choice([\"Yes\",\"No\"], n, p=[0.45,0.55]),\n",
        "        \"Dependents\": rng.choice([\"Yes\",\"No\"], n, p=[0.25,0.75]),\n",
        "        \"tenure\": rng.integers(0, 72, n),\n",
        "        \"PhoneService\": rng.choice([\"Yes\",\"No\"], n, p=[0.9,0.1]),\n",
        "        \"MultipleLines\": rng.choice([\"Yes\",\"No\",\"No phone service\"], n),\n",
        "        \"InternetService\": rng.choice([\"DSL\",\"Fiber optic\",\"No\"], n, p=[0.4,0.45,0.15]),\n",
        "        \"OnlineSecurity\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"OnlineBackup\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"DeviceProtection\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"TechSupport\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"StreamingTV\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"StreamingMovies\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"Contract\": rng.choice([\"Month-to-month\", \"One year\", \"Two year\"], n, p=[0.6,0.2,0.2]),\n",
        "        \"PaperlessBilling\": rng.choice([\"Yes\",\"No\"], n),\n",
        "        \"PaymentMethod\": rng.choice([\n",
        "            \"Electronic check\",\"Mailed check\",\"Bank transfer (automatic)\",\"Credit card (automatic)\"\n",
        "        ], n),\n",
        "        \"MonthlyCharges\": np.round(rng.uniform(18.0, 120.0, n), 2),\n",
        "    })\n",
        "    df[\"TotalCharges\"] = np.round(df[\"MonthlyCharges\"] * df[\"tenure\"] + rng.uniform(0, 50, n), 2)\n",
        "    # churn probability function\n",
        "    churn_prob = (\n",
        "        0.35 - 0.004 * df[\"tenure\"] +\n",
        "        np.where(df[\"Contract\"] == \"Month-to-month\", 0.2, -0.05) +\n",
        "        np.where(df[\"PaymentMethod\"] == \"Electronic check\", 0.05, 0)\n",
        "    )\n",
        "    churn_prob = np.clip(churn_prob, 0.01, 0.9)\n",
        "    df[\"Churn\"] = np.where(rng.random(n) < churn_prob, \"Yes\", \"No\")\n",
        "    df.to_csv(path, index=False)\n",
        "    return df\n",
        "\n",
        "def load_data(path: Path = DATA_FILE) -> pd.DataFrame:\n",
        "    if path.exists():\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"[INFO] Loaded dataset from {path} shape={df.shape}\")\n",
        "    else:\n",
        "        print(f\"[WARN] Dataset not found at {path}. Creating synthetic dataset for development.\")\n",
        "        df = create_synthetic_telco(path)\n",
        "        print(f\"[INFO] Synthetic dataset created at {path} shape={df.shape}\")\n",
        "    # normalize column names\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    return df\n",
        "\n",
        "# ---------------------------\n",
        "# Exploratory Data Analysis\n",
        "# ---------------------------\n",
        "def eda_summary(df: pd.DataFrame, target: str = \"Churn\") -> Dict[str, Any]:\n",
        "    df = df.copy()\n",
        "    summary = {}\n",
        "    summary[\"shape\"] = df.shape\n",
        "    summary[\"columns\"] = df.columns.tolist()\n",
        "    summary[\"dtypes\"] = df.dtypes.apply(lambda x: str(x)).to_dict()\n",
        "    summary[\"missing\"] = df.isnull().sum().to_dict()\n",
        "    if target in df.columns:\n",
        "        summary[\"class_distribution\"] = df[target].value_counts().to_dict()\n",
        "    # numeric stats\n",
        "    summary[\"numeric\"] = df.select_dtypes(include=[np.number]).describe().to_dict()\n",
        "    # correlation\n",
        "    try:\n",
        "        corr = df.select_dtypes(include=[np.number]).corr()\n",
        "        summary[\"correlation\"] = corr.fillna(0).to_dict()\n",
        "    except Exception:\n",
        "        summary[\"correlation\"] = {}\n",
        "    # save summary\n",
        "    save_json(summary, REPORTS_DIR / \"eda_summary.json\")\n",
        "    return summary\n",
        "\n",
        "def eda_plots(df: pd.DataFrame, target: str = \"Churn\"):\n",
        "    # churn distribution\n",
        "    if target in df.columns:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        df[target].value_counts().plot(kind=\"bar\")\n",
        "        plt.title(\"Churn distribution\")\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIG_DIR / \"churn_distribution.png\")\n",
        "        plt.close()\n",
        "\n",
        "    # Numeric histograms for selected columns\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    for c in [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]:\n",
        "        if c in df.columns:\n",
        "            plt.figure(figsize=(6,4))\n",
        "            df[c].dropna().hist(bins=40)\n",
        "            plt.title(f\"{c} distribution\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / f\"{c}_hist.png\")\n",
        "            plt.close()\n",
        "\n",
        "    # Correlation heatmap (numeric)\n",
        "    try:\n",
        "        import seaborn as sns\n",
        "        num = df.select_dtypes(include=[np.number])\n",
        "        if num.shape[1] > 1:\n",
        "            plt.figure(figsize=(10,8))\n",
        "            sns.heatmap(num.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / \"numeric_corr_heatmap.png\")\n",
        "            plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------------------------\n",
        "# Preprocessing & Feature Engineering\n",
        "# ---------------------------\n",
        "def preprocess(df: pd.DataFrame, target: str = \"Churn\") -> Tuple[pd.DataFrame, np.ndarray, ColumnTransformer]:\n",
        "    df = df.copy()\n",
        "    if \"customerID\" in df.columns:\n",
        "        df = df.drop(columns=[\"customerID\"])\n",
        "\n",
        "    # Convert TotalCharges to numeric (Telco dataset has some spaces)\n",
        "    if \"TotalCharges\" in df.columns:\n",
        "        df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "    # Remove rows with missing target\n",
        "    df = df.dropna(subset=[target])\n",
        "\n",
        "    # Identify numeric and categorical\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    if target in categorical_cols:\n",
        "        categorical_cols.remove(target)\n",
        "    if target in numeric_cols:\n",
        "        numeric_cols.remove(target)\n",
        "\n",
        "    # Simple feature engineering: tenure bins\n",
        "    if \"tenure\" in df.columns:\n",
        "        df[\"tenure_bin\"] = pd.cut(df[\"tenure\"].fillna(-1), bins=[-1,0,12,24,48,72], labels=[\"0\",\"1-12\",\"13-24\",\"25-48\",\"49-72\"])\n",
        "        # add to categorical\n",
        "        categorical_cols.append(\"tenure_bin\")\n",
        "\n",
        "    # Fill NA in categorical with 'Missing'\n",
        "    df[categorical_cols] = df[categorical_cols].fillna(\"Missing\")\n",
        "\n",
        "    # Prepare transformers\n",
        "    numeric_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    categorical_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False))\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"num\", numeric_transformer, numeric_cols),\n",
        "        (\"cat\", categorical_transformer, categorical_cols)\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    # Label encode target\n",
        "    y = df[target].apply(lambda x: 1 if str(x).strip().lower() in [\"yes\", \"1\", \"true\", \"y\"] else 0).values\n",
        "\n",
        "    X = df.drop(columns=[target]).copy()\n",
        "\n",
        "    # Save metadata for report\n",
        "    meta = {\n",
        "        \"numeric_cols\": numeric_cols,\n",
        "        \"categorical_cols\": categorical_cols,\n",
        "        \"final_shape_before_transform\": X.shape\n",
        "    }\n",
        "    save_json(meta, REPORTS_DIR / \"preprocessing_meta.json\")\n",
        "\n",
        "    return X, y, preprocessor\n",
        "\n",
        "# ---------------------------\n",
        "# Feature selection helpers\n",
        "# ---------------------------\n",
        "def compute_mutual_info(X: pd.DataFrame, y: np.ndarray, preprocessor: ColumnTransformer, top_k: int = 20) -> List[Tuple[str,float]]:\n",
        "    # Apply preprocessor to get numeric features (but OneHotEncoder will expand features; we extract names)\n",
        "    # Fit preprocessor on X to allow get_feature_names_out (sklearn >=1.0)\n",
        "    preprocessor.fit(X)\n",
        "    try:\n",
        "        # Build feature names\n",
        "        num_cols = []\n",
        "        cat_cols = []\n",
        "        for name, trans, cols in preprocessor.transformers_:\n",
        "            if name == \"num\":\n",
        "                num_cols = cols\n",
        "            if name == \"cat\":\n",
        "                cat_cols = cols\n",
        "        # get transformed feature count & names\n",
        "        ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "        ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
        "        feature_names = list(num_cols) + ohe_names\n",
        "    except Exception:\n",
        "        # fallback: use numeric columns only\n",
        "        feature_names = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    # Transform X to numeric matrix for mutual info\n",
        "    X_num = preprocessor.transform(X)\n",
        "    mi = mutual_info_classif(X_num, y, random_state=RANDOM_STATE)\n",
        "    mi_series = pd.Series(mi, index=feature_names).sort_values(ascending=False)\n",
        "    top = list(mi_series.head(top_k).items())\n",
        "    # save\n",
        "    save_json({k: float(v) for k,v in top}, REPORTS_DIR / \"mutual_info_top.json\")\n",
        "    return top\n",
        "\n",
        "# ---------------------------\n",
        "# Train / Evaluate utilities\n",
        "# ---------------------------\n",
        "def classification_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray = None) -> Dict[str, Any]:\n",
        "    metrics = {}\n",
        "    metrics[\"accuracy\"] = float(accuracy_score(y_true, y_pred))\n",
        "    metrics[\"precision\"] = float(precision_score(y_true, y_pred, zero_division=0))\n",
        "    metrics[\"recall\"] = float(recall_score(y_true, y_pred, zero_division=0))\n",
        "    metrics[\"f1\"] = float(f1_score(y_true, y_pred, zero_division=0))\n",
        "    if y_proba is not None:\n",
        "        try:\n",
        "            metrics[\"roc_auc\"] = float(roc_auc_score(y_true, y_proba))\n",
        "        except Exception:\n",
        "            metrics[\"roc_auc\"] = None\n",
        "        try:\n",
        "            metrics[\"pr_auc\"] = float(average_precision_score(y_true, y_proba))\n",
        "        except Exception:\n",
        "            metrics[\"pr_auc\"] = None\n",
        "    else:\n",
        "        metrics[\"roc_auc\"] = None\n",
        "        metrics[\"pr_auc\"] = None\n",
        "    metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred).tolist()\n",
        "    return metrics\n",
        "\n",
        "def plot_roc_pr(y_test: np.ndarray, y_proba: np.ndarray, prefix: str):\n",
        "    # ROC\n",
        "    try:\n",
        "        from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
        "        plt.plot([0,1],[0,1],\"--\", color=\"gray\")\n",
        "        plt.xlabel(\"FPR\")\n",
        "        plt.ylabel(\"TPR\")\n",
        "        plt.title(f\"ROC Curve - {prefix}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIG_DIR / f\"{prefix}_roc.png\")\n",
        "        plt.close()\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(recall, precision)\n",
        "        plt.xlabel(\"Recall\")\n",
        "        plt.ylabel(\"Precision\")\n",
        "        plt.title(f\"Precision-Recall Curve - {prefix}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIG_DIR / f\"{prefix}_pr.png\")\n",
        "        plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# ---------------------------\n",
        "# Modeling: training wrappers\n",
        "# ---------------------------\n",
        "def train_baselines(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    # Dummy baseline\n",
        "    dummy_pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", DummyClassifier(strategy=\"most_frequent\"))])\n",
        "    dummy_pipe.fit(X_train, y_train)\n",
        "    y_pred = dummy_pipe.predict(X_test)\n",
        "    y_proba = dummy_pipe.predict_proba(X_test)[:,1] if hasattr(dummy_pipe, \"predict_proba\") else None\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(dummy_pipe, MODEL_DIR / \"baseline_dummy.joblib\")\n",
        "    return {\"dummy\": metrics}\n",
        "\n",
        "def train_logistic(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    y_proba = pipe.predict_proba(X_test)[:,1]\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(pipe, MODEL_DIR / \"logistic_pipeline.joblib\")\n",
        "    return {\"logistic\": metrics, \"model\": pipe}\n",
        "\n",
        "def train_random_forest(X_train, y_train, X_test, y_test, preprocessor, random_search=False):\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))])\n",
        "    if random_search:\n",
        "        param_dist = {\n",
        "            \"clf__n_estimators\": [100, 250, 500],\n",
        "            \"clf__max_depth\": [6, 10, 20, None],\n",
        "            \"clf__min_samples_split\": [2,5,10],\n",
        "            \"clf__max_features\": [\"sqrt\", \"log2\", 0.2, 0.5]\n",
        "        }\n",
        "        rs = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=20, scoring=\"f1\", cv=3, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "        rs.fit(X_train, y_train)\n",
        "        best = rs.best_estimator_\n",
        "    else:\n",
        "        pipe.fit(X_train, y_train)\n",
        "        best = pipe\n",
        "    y_pred = best.predict(X_test)\n",
        "    y_proba = best.predict_proba(X_test)[:,1]\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(best, MODEL_DIR / \"random_forest.joblib\")\n",
        "    return {\"random_forest\": metrics, \"model\": best}\n",
        "\n",
        "def train_xgboost(X_train, y_train, X_test, y_test, preprocessor, random_search=False):\n",
        "    if not XGBOOST_AVAILABLE:\n",
        "        print(\"[WARN] XGBoost not available; skip.\")\n",
        "        return None\n",
        "    # use sklearn wrapper\n",
        "    clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", clf)])\n",
        "    if random_search:\n",
        "        param_dist = {\n",
        "            \"clf__n_estimators\": [100, 250, 500],\n",
        "            \"clf__max_depth\": [3,6,10],\n",
        "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
        "            \"clf__subsample\": [0.6, 0.8, 1.0]\n",
        "        }\n",
        "        rs = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=20, scoring=\"f1\", cv=3, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "        rs.fit(X_train, y_train)\n",
        "        best = rs.best_estimator_\n",
        "    else:\n",
        "        pipe.fit(X_train, y_train)\n",
        "        best = pipe\n",
        "    y_pred = best.predict(X_test)\n",
        "    y_proba = best.predict_proba(X_test)[:,1]\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(best, MODEL_DIR / \"xgboost.joblib\")\n",
        "    return {\"xgboost\": metrics, \"model\": best}\n",
        "\n",
        "def train_hist_gbm(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    # fallback to sklearn's HistGradientBoostingClassifier\n",
        "    try:\n",
        "        from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "        clf = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "        pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", clf)])\n",
        "        pipe.fit(X_train, y_train)\n",
        "        y_pred = pipe.predict(X_test)\n",
        "        # HGB does not always have predict_proba for some versions; handle gracefully\n",
        "        y_proba = pipe.predict_proba(X_test)[:,1] if hasattr(pipe, \"predict_proba\") else None\n",
        "        metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "        joblib.dump(pipe, MODEL_DIR / \"hist_gbm.joblib\")\n",
        "        return {\"hist_gbm\": metrics, \"model\": pipe}\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] HistGradientBoosting unavailable:\", e)\n",
        "        return None\n",
        "\n",
        "def train_nn_model(X_train, y_train, X_test, y_test, preprocessor, epochs=30, batch_size=64):\n",
        "    if not KERAS_AVAILABLE:\n",
        "        print(\"[WARN] Keras/TensorFlow not available; skipping NN.\")\n",
        "        return None\n",
        "    # Preprocess to dense numpy\n",
        "    X_train_p = preprocessor.fit_transform(X_train)\n",
        "    X_test_p = preprocessor.transform(X_test)\n",
        "    input_dim = X_train_p.shape[1]\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[keras.metrics.AUC(name=\"auc\")])\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True)\n",
        "    history = model.fit(X_train_p, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, callbacks=[early_stop], verbose=0)\n",
        "\n",
        "    y_proba = model.predict(X_test_p).ravel()\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "\n",
        "    # Save model and preprocessor\n",
        "    model_path = MODEL_DIR / \"nn_model.keras\"\n",
        "    model.save(model_path)\n",
        "    joblib.dump(preprocessor, MODEL_DIR / \"nn_preprocessor.joblib\")\n",
        "    # Save history\n",
        "    hist_path = REPORTS_DIR / \"nn_history.json\"\n",
        "    save_json({k: [float(v) for v in vals] for k, vals in history.history.items()}, hist_path)\n",
        "\n",
        "    return {\"neural_network\": metrics, \"model\": (model, preprocessor)}\n",
        "\n",
        "# ---------------------------\n",
        "# Explainability (SHAP / LIME)\n",
        "# ---------------------------\n",
        "def explain_with_shap(model_pipe, X_train: pd.DataFrame, sample_X_test: pd.DataFrame, model_name: str):\n",
        "    if not SHAP_AVAILABLE:\n",
        "        print(\"[WARN] SHAP not installed; install shap to get explanations.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"[INFO] Generating SHAP explanations for {model_name}...\")\n",
        "\n",
        "    # extract the underlying model and preprocessor\n",
        "    if isinstance(model_pipe, Pipeline):\n",
        "        pre = model_pipe.named_steps.get(\"prep\")\n",
        "        clf = model_pipe.named_steps.get(\"clf\")\n",
        "    else:\n",
        "        # for (model, preprocessor) tuple\n",
        "        try:\n",
        "            clf, pre = model_pipe\n",
        "        except Exception:\n",
        "            print(\"[WARN] Unknown model_pipe structure for SHAP\")\n",
        "            return None\n",
        "\n",
        "    X_sample_trans = pre.transform(sample_X_test)\n",
        "    # Use TreeExplainer for tree-based models\n",
        "    try:\n",
        "        explainer = shap.Explainer(clf)\n",
        "        shap_values = explainer(X_sample_trans)\n",
        "    except Exception:\n",
        "        # fallback: KernelExplainer (slower)\n",
        "        explainer = shap.KernelExplainer(lambda x: clf.predict_proba(x)[:,1], shap.sample(X_sample_trans, 100))\n",
        "        shap_values = explainer.shap_values(X_sample_trans[:100])\n",
        "    # Save a small summary plot\n",
        "    try:\n",
        "        shap.summary_plot(shap_values, X_sample_trans, show=False)\n",
        "        plt.savefig(FIG_DIR / f\"shap_summary_{model_name}.png\", bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return shap_values\n",
        "\n",
        "def explain_with_lime(model_pipe, X_train: pd.DataFrame, sample_row: pd.Series):\n",
        "    if not LIME_AVAILABLE:\n",
        "        print(\"[WARN] LIME not installed; install lime to get local explanations.\")\n",
        "        return None\n",
        "    # LIME needs numpy arrays and feature names\n",
        "    pre = model_pipe.named_steps.get(\"prep\")\n",
        "    clf = model_pipe.named_steps.get(\"clf\")\n",
        "    X_train_p = pre.transform(X_train)\n",
        "    feature_names = []\n",
        "    # try to get OHE feature names\n",
        "    try:\n",
        "        cat_step = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "        cat_cols = pre.transformers_[1][2]\n",
        "        ohe_names = list(cat_step.get_feature_names_out(cat_cols))\n",
        "        num_cols = pre.transformers_[0][2]\n",
        "        feature_names = list(num_cols) + ohe_names\n",
        "    except Exception:\n",
        "        # fallback\n",
        "        feature_names = [f\"f{i}\" for i in range(X_train_p.shape[1])]\n",
        "    explainer = lime_tabular.LimeTabularExplainer(X_train_p, feature_names=feature_names, class_names=[\"No\",\"Yes\"], discretize_continuous=True)\n",
        "    row_p = pre.transform(pd.DataFrame([sample_row]))\n",
        "    exp = explainer.explain_instance(row_p[0], clf.predict_proba, num_features=10)\n",
        "    return exp\n",
        "\n",
        "# ---------------------------\n",
        "# Post-deployment: Drift detection (PSI)\n",
        "# ---------------------------\n",
        "def population_stability_index(expected: np.ndarray, actual: np.ndarray, buckets: int = 10) -> float:\n",
        "    \"\"\"Compute PSI between expected and actual arrays (numeric)\"\"\"\n",
        "    expected = np.array(expected).ravel()\n",
        "    actual = np.array(actual).ravel()\n",
        "    # build quantile bins from expected\n",
        "    breakpoints = np.percentile(expected, np.linspace(0, 100, buckets+1))\n",
        "    expected_perc = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
        "    actual_perc = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
        "    # avoid zeros\n",
        "    eps = 1e-6\n",
        "    expected_perc = np.where(expected_perc == 0, eps, expected_perc)\n",
        "    actual_perc = np.where(actual_perc == 0, eps, actual_perc)\n",
        "    psi = np.sum((expected_perc - actual_perc) * np.log(expected_perc / actual_perc))\n",
        "    return float(psi)\n",
        "\n",
        "# ---------------------------\n",
        "# Anomaly detection (IsolationForest)\n",
        "# ---------------------------\n",
        "def build_anomaly_detector(X: pd.DataFrame, preprocessor: ColumnTransformer):\n",
        "    X_p = preprocessor.fit_transform(X)\n",
        "    iso = IsolationForest(contamination=0.01, random_state=RANDOM_STATE)\n",
        "    iso.fit(X_p)\n",
        "    joblib.dump(iso, MODEL_DIR / \"isolation_forest.joblib\")\n",
        "    return iso\n",
        "\n",
        "def score_anomalies(iso: IsolationForest, X: pd.DataFrame, preprocessor: ColumnTransformer):\n",
        "    X_p = preprocessor.transform(X)\n",
        "    scores = -iso.decision_function(X_p)  # higher = more anomalous\n",
        "    return scores\n",
        "\n",
        "# ---------------------------\n",
        "# Real-time inference helper\n",
        "# ---------------------------\n",
        "def build_predict_fn(model_pipe) -> callable:\n",
        "    \"\"\"\n",
        "    Returns a function predict_fn(dict_row)->(pred, proba)\n",
        "    \"\"\"\n",
        "    def predict_fn(input_dict: Dict[str, Any]):\n",
        "        row = pd.DataFrame([input_dict])\n",
        "        if isinstance(model_pipe, Pipeline):\n",
        "            proba = model_pipe.predict_proba(row)[:,1]\n",
        "            pred = (proba >= 0.5).astype(int)\n",
        "            return int(pred[0]), float(proba[0])\n",
        "        else:\n",
        "            # tuple (model, preprocessor) expected for NN\n",
        "            try:\n",
        "                model, pre = model_pipe\n",
        "                Xp = pre.transform(row)\n",
        "                proba = model.predict(Xp).ravel()\n",
        "                pred = (proba >= 0.5).astype(int)\n",
        "                return int(pred[0]), float(proba[0])\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(\"Unknown model_pipe format for prediction\") from e\n",
        "\n",
        "    return predict_fn\n",
        "\n",
        "# ---------------------------\n",
        "# Orchestrator: run full experiment\n",
        "# ---------------------------\n",
        "def run_full_experiment(random_search: bool = True, balance: bool = True):\n",
        "    print(\"[INFO] Loading data\")\n",
        "    df = load_data()\n",
        "    ensure_df_has_target(df, \"Churn\")\n",
        "    eda_summary(df, \"Churn\")\n",
        "    eda_plots(df, \"Churn\")\n",
        "\n",
        "    print(\"[INFO] Preprocessing data\")\n",
        "    X, y, preprocessor = preprocess(df, target=\"Churn\")\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "    # Handle imbalance with SMOTE if available and requested\n",
        "    if IMBLEARN_AVAILABLE and balance:\n",
        "        print(\"[INFO] Applying SMOTE to training data\")\n",
        "        X_train_prep = preprocessor.fit_transform(X_train)\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_res, y_res = sm.fit_resample(X_train_prep, y_train)\n",
        "        # after resampling, we cannot easily inverse-transform to DataFrame; so for model training we will\n",
        "        # fit models on pipelines that accept raw X_train and pipeline internal preprocessor will be re-fit.\n",
        "        # To keep things consistent, we will re-fit pipeline directly on raw X_train but with class_weight if needed.\n",
        "        use_smote = True\n",
        "    else:\n",
        "        X_res, y_res = None, None\n",
        "        use_smote = False\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Baseline\n",
        "    print(\"[INFO] Training baseline models\")\n",
        "    results.update(train_baselines(X_train, y_train, X_test, y_test, preprocessor))\n",
        "\n",
        "    # Logistic\n",
        "    print(\"[INFO] Training Logistic Regression\")\n",
        "    log_res = train_logistic(X_train, y_train, X_test, y_test, preprocessor)\n",
        "    results.update({k:v for k,v in log_res.items() if k != \"model\"})\n",
        "\n",
        "    # Random Forest\n",
        "    print(\"[INFO] Training Random Forest\")\n",
        "    rf_res = train_random_forest(X_train, y_train, X_test, y_test, preprocessor, random_search=random_search)\n",
        "    results.update({k:v for k,v in rf_res.items() if k != \"model\"})\n",
        "    rf_model = rf_res.get(\"model\")\n",
        "\n",
        "    # XGBoost or HistGradientBoosting\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        print(\"[INFO] Training XGBoost\")\n",
        "        xgb_res = train_xgboost(X_train, y_train, X_test, y_test, preprocessor, random_search=random_search)\n",
        "        if xgb_res is not None:\n",
        "            results.update({k:v for k,v in xgb_res.items() if k != \"model\"})\n",
        "    else:\n",
        "        print(\"[INFO] Training HistGradientBoosting (fallback)\")\n",
        "        hgb_res = train_hist_gbm(X_train, y_train, X_test, y_test, preprocessor)\n",
        "        if hgb_res:\n",
        "            results.update({k:v for k,v in hgb_res.items() if k != \"model\"})\n",
        "\n",
        "    # Neural Network\n",
        "    print(\"[INFO] Training Neural Network (may take a while)\")\n",
        "    nn_res = train_nn_model(X_train, y_train, X_test, y_test, preprocessor, epochs=30, batch_size=128)\n",
        "    if nn_res:\n",
        "        results.update({k:v for k,v in nn_res.items() if k != \"model\"})\n",
        "\n",
        "    # Compare results and choose best by f1 or roc_auc\n",
        "    save_json(results, REPORTS_DIR / \"model_metrics_summary.json\")\n",
        "\n",
        "    # Example: use random forest pipeline as final model if available\n",
        "    try:\n",
        "        final_model = rf_model\n",
        "    except Exception:\n",
        "        final_model = None\n",
        "\n",
        "    if final_model is None:\n",
        "        print(\"[WARN] No final model found; using logistic pipeline as final fallback.\")\n",
        "        try:\n",
        "            final_model = joblib.load(MODEL_DIR / \"logistic_pipeline.joblib\")\n",
        "        except Exception:\n",
        "            final_model = None\n",
        "\n",
        "    if final_model is not None:\n",
        "        print(\"[INFO] Generating explanations for final model (SHAP if available)\")\n",
        "        # pick sample of test set rows for explainability\n",
        "        sample_X_test = X_test.sample(n=min(200, max(1, len(X_test))), random_state=RANDOM_STATE)\n",
        "        if SHAP_AVAILABLE:\n",
        "            try:\n",
        "                shap_values = explain_with_shap(final_model, X_train, sample_X_test, \"final_model\")\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] SHAP explanation failed:\", e)\n",
        "        if LIME_AVAILABLE:\n",
        "            try:\n",
        "                exp = explain_with_lime(final_model, X_train, sample_X_test.iloc[0])\n",
        "                # save lime explanation as text\n",
        "                with open(FIG_DIR / \"lime_exp.txt\", \"w\") as f:\n",
        "                    f.write(str(exp.as_list()))\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] LIME explanation failed:\", e)\n",
        "\n",
        "        # Build predict function and save\n",
        "        predict_fn = build_predict_fn(final_model)\n",
        "        # Example prediction saved\n",
        "        example_row = X_test.iloc[0].to_dict()\n",
        "        pred, proba = predict_fn(example_row)\n",
        "        save_json({\"example_prediction\": {\"pred\": pred, \"proba\": proba, \"row\": example_row}}, REPORTS_DIR / \"example_prediction.json\")\n",
        "\n",
        "    # Build anomaly detector on whole training data\n",
        "    try:\n",
        "        print(\"[INFO] Building anomaly detector\")\n",
        "        iso = build_anomaly_detector(X, preprocessor)\n",
        "        scores = score_anomalies(iso, X_test, preprocessor)\n",
        "        # attach scores to sample and save\n",
        "        out_df = X_test.copy()\n",
        "        out_df[\"anomaly_score\"] = scores\n",
        "        out_df.head(20).to_csv(REPORTS_DIR / \"anomaly_scores_sample.csv\", index=False)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Anomaly detector failed:\", e)\n",
        "\n",
        "    # Post-deployment: compute PSI between training probability distribution and test probability distribution for a model (if computed)\n",
        "    try:\n",
        "        # use logistic model proba if available\n",
        "        try:\n",
        "            logistic_pipe = joblib.load(MODEL_DIR / \"logistic_pipeline.joblib\")\n",
        "            proba_train = logistic_pipe.predict_proba(X_train)[:,1]\n",
        "            proba_test = logistic_pipe.predict_proba(X_test)[:,1]\n",
        "            psi = population_stability_index(proba_train, proba_test, buckets=10)\n",
        "            save_json({\"psi\": psi}, REPORTS_DIR / \"post_deployment_psi.json\")\n",
        "            print(f\"[INFO] PSI computed: {psi:.5f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] PSI computation failed:\", e)\n",
        "\n",
        "    print(\"[INFO] Full experiment complete. Outputs written to:\", OUT_DIR)\n",
        "    return results\n",
        "\n",
        "# ---------------------------\n",
        "# Run when executed directly\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    results = run_full_experiment(random_search=True, balance=True)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"[INFO] Total time elapsed: {elapsed/60:.2f} minutes\")\n",
        "    # print brief results\n",
        "    try:\n",
        "        print(json.dumps(results, indent=2))\n",
        "    except Exception:\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "VXz6gvevlDv1",
        "outputId": "fe690258-ecfe-46b1-c587-f02080c8cc25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Telco-Customer-Churn.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-579426206.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Telco-Customer-Churn.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Telco-Customer-Churn.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UuJKL0S_lRHY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}