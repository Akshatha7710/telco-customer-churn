{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN6MWkk5uJ7kP9GOK6iviMk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshatha7710/telco-customer-churn/blob/main/telco_customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "MZh7DS0FkwvL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "157c2c15-d3e7-4840-9bc7-6eac4795f374"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-696595547.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m     ) from _err\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m from pandas._config import (\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mget_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mset_option\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;34m\"warn_copy_on_write\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m ]\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdates\u001b[0m  \u001b[0;31m# pyright: ignore[reportUnusedImport]  # noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m from pandas._config.config import (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_config/config.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m from pandas._typing import (\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/_typing.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBitGenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__dir__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mpublic_symbols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'testing'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         public_symbols -= {\n\u001b[1;32m    339\u001b[0m             \u001b[0;34m\"core\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"matrixlib\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;31m# add these for module-freeze analysis (like PyInstaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_common\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_bounded_integers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/numpy/random/_pickle.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmtrand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomState\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_philox\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhilox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_pcg64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCG64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCG64DXSM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_sfc64\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSFC64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mnumpy/random/mtrand.pyx\u001b[0m in \u001b[0;36minit numpy.random.mtrand\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject"
          ]
        }
      ],
      "source": [
        "# ---------------------------\n",
        "# Core imports\n",
        "# ---------------------------\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# ---------------------------\n",
        "# Scikit-learn imports\n",
        "# ---------------------------\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, average_precision_score, confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# ---------------------------\n",
        "# Optional libraries with availability flags\n",
        "# ---------------------------\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    KERAS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from lime import lime_tabular\n",
        "    LIME_AVAILABLE = True\n",
        "except ImportError:\n",
        "    LIME_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import joblib\n",
        "except ImportError:\n",
        "    joblib = None\n",
        "\n",
        "# ---------------------------\n",
        "# Misc\n",
        "# ---------------------------\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------------------------\n",
        "# Random state for reproducibility\n",
        "# ---------------------------\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "if KERAS_AVAILABLE:\n",
        "    tf.random.set_seed(RANDOM_STATE)\n",
        "\n",
        "print(\"[INFO] All libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Utilities / Data Loading / Synthetic\n",
        "# ---------------------------\n",
        "def save_json(obj: Any, path: Path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def create_synthetic_telco(path: Path, n=2000) -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(RANDOM_STATE)\n",
        "    df = pd.DataFrame({\n",
        "        \"customerID\": [f\"CUST{100000+i}\" for i in range(n)],\n",
        "        \"gender\": rng.choice([\"Female\", \"Male\"], n),\n",
        "        \"SeniorCitizen\": rng.choice([0,1], n, p=[0.85,0.15]),\n",
        "        \"Partner\": rng.choice([\"Yes\",\"No\"], n, p=[0.45,0.55]),\n",
        "        \"Dependents\": rng.choice([\"Yes\",\"No\"], n, p=[0.25,0.75]),\n",
        "        \"tenure\": rng.integers(0, 72, n),\n",
        "        \"PhoneService\": rng.choice([\"Yes\",\"No\"], n, p=[0.9,0.1]),\n",
        "        \"MultipleLines\": rng.choice([\"Yes\",\"No\",\"No phone service\"], n),\n",
        "        \"InternetService\": rng.choice([\"DSL\",\"Fiber optic\",\"No\"], n, p=[0.4,0.45,0.15]),\n",
        "        \"OnlineSecurity\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"OnlineBackup\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"DeviceProtection\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"TechSupport\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"StreamingTV\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"StreamingMovies\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"Contract\": rng.choice([\"Month-to-month\", \"One year\", \"Two year\"], n, p=[0.6,0.2,0.2]),\n",
        "        \"PaperlessBilling\": rng.choice([\"Yes\",\"No\"], n),\n",
        "        \"PaymentMethod\": rng.choice([\n",
        "            \"Electronic check\",\"Mailed check\",\"Bank transfer (automatic)\",\"Credit card (automatic)\"\n",
        "        ], n),\n",
        "        \"MonthlyCharges\": np.round(rng.uniform(18.0, 120.0, n), 2),\n",
        "    })\n",
        "    df[\"TotalCharges\"] = np.round(df[\"MonthlyCharges\"] * df[\"tenure\"] + rng.uniform(0, 50, n), 2)\n",
        "    churn_prob = (\n",
        "        0.35 - 0.004 * df[\"tenure\"] +\n",
        "        np.where(df[\"Contract\"] == \"Month-to-month\", 0.2, -0.05) +\n",
        "        np.where(df[\"PaymentMethod\"] == \"Electronic check\", 0.05, 0)\n",
        "    )\n",
        "    churn_prob = np.clip(churn_prob, 0.01, 0.9)\n",
        "    df[\"Churn\"] = np.where(rng.random(n) < churn_prob, \"Yes\", \"No\")\n",
        "    df.to_csv(path, index=False)\n",
        "    return df\n",
        "\n",
        "def load_data(path: Path = DATA_FILE) -> pd.DataFrame:\n",
        "    if path.exists():\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"[INFO] Loaded dataset from {path} shape={df.shape}\")\n",
        "    else:\n",
        "        print(f\"[WARN] Dataset not found at {path}. Creating synthetic dataset for development.\")\n",
        "        df = create_synthetic_telco(path)\n",
        "        print(f\"[INFO] Synthetic dataset created at {path} shape={df.shape}\")\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "vpoqnj27pC7c",
        "outputId": "1731f489-78d7-4adf-d434-5c8e1a778db7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1636387821.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_synthetic_telco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_rng\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRANDOM_STATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     df = pd.DataFrame({\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Exploratory Data Analysis\n",
        "# ---------------------------\n",
        "def run_eda(df: pd.DataFrame):\n",
        "    print(\"[EDA] Dataset shape:\", df.shape)\n",
        "    display(df.head())\n",
        "    display(df.describe(include=\"all\"))\n",
        "    # Missing heatmap\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.heatmap(df.isnull(), cbar=False)\n",
        "    plt.title(\"Missing values\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / \"missing_heatmap.png\")\n",
        "    plt.close()\n",
        "    # Churn distribution\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.countplot(x=\"Churn\", data=df)\n",
        "    plt.title(\"Churn distribution\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(FIG_DIR / \"churn_distribution.png\")\n",
        "    plt.close()\n",
        "    # Numeric histograms\n",
        "    for c in [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]:\n",
        "        if c in df.columns:\n",
        "            plt.figure(figsize=(6,4))\n",
        "            df[c].dropna().hist(bins=40)\n",
        "            plt.title(f\"{c} distribution\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / f\"{c}_hist.png\")\n",
        "            plt.close()\n",
        "    # Correlation heatmap\n",
        "    try:\n",
        "        plt.figure(figsize=(8,6))\n",
        "        sns.heatmap(df.select_dtypes(include=[\"number\"]).corr(), cmap=\"coolwarm\", annot=False)\n",
        "        plt.title(\"Numeric correlations\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIG_DIR / \"numeric_corr.png\")\n",
        "        plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "# Run EDA now (loads data)\n",
        "df = load_data()\n",
        "run_eda(df)\n"
      ],
      "metadata": {
        "id": "MqB4fV4_pYMp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Preprocessing & Feature Engineering\n",
        "# ---------------------------\n",
        "def preprocess(df: pd.DataFrame, target: str = \"Churn\") -> Tuple[pd.DataFrame, np.ndarray, ColumnTransformer]:\n",
        "    df = df.copy()\n",
        "    if \"customerID\" in df.columns:\n",
        "        df = df.drop(columns=[\"customerID\"])\n",
        "    if \"TotalCharges\" in df.columns:\n",
        "        df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[target])\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "    if target in categorical_cols:\n",
        "        categorical_cols.remove(target)\n",
        "    if target in numeric_cols:\n",
        "        numeric_cols.remove(target)\n",
        "    # tenure bin\n",
        "    if \"tenure\" in df.columns:\n",
        "        df[\"tenure_bin\"] = pd.cut(df[\"tenure\"].fillna(-1), bins=[-1,0,12,24,48,72],\n",
        "                                  labels=[\"0\",\"1-12\",\"13-24\",\"25-48\",\"49-72\"])\n",
        "        if \"tenure_bin\" not in categorical_cols:\n",
        "            categorical_cols.append(\"tenure_bin\")\n",
        "    # fill categorical missing safely\n",
        "    for col in categorical_cols:\n",
        "        if df[col].dtype.name == \"category\":\n",
        "            if \"Missing\" not in df[col].cat.categories:\n",
        "                df[col] = df[col].cat.add_categories([\"Missing\"])\n",
        "        df[col] = df[col].fillna(\"Missing\")\n",
        "    # numeric pipeline\n",
        "    numeric_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "    # categorical pipeline: be compatible with sklearn versions\n",
        "    try:\n",
        "        onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
        "    except TypeError:\n",
        "        onehot = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
        "    categorical_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", onehot)\n",
        "    ])\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"num\", numeric_transformer, numeric_cols),\n",
        "        (\"cat\", categorical_transformer, categorical_cols)\n",
        "    ], remainder=\"drop\")\n",
        "    y = df[target].apply(lambda x: 1 if str(x).strip().lower() in [\"yes\",\"1\",\"true\",\"y\"] else 0).values\n",
        "    X = df.drop(columns=[target]).copy()\n",
        "    save_json({\"numeric_cols\": numeric_cols, \"categorical_cols\": categorical_cols, \"shape\": X.shape}, REPORTS_DIR / \"preprocessing_meta.json\")\n",
        "    return X, y, preprocessor\n",
        "\n",
        "X, y, preprocessor = preprocess(df, target=\"Churn\")\n",
        "print(\"[INFO] Preprocessed: X shape before transform:\", X.shape)\n"
      ],
      "metadata": {
        "id": "JXfL3xUQplKs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Feature selection helpers\n",
        "# ---------------------------\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "def compute_mutual_info(X: pd.DataFrame, y: np.ndarray, preprocessor: ColumnTransformer, top_k: int = 20):\n",
        "    preprocessor.fit(X)\n",
        "    # attempt to build transformed feature names\n",
        "    try:\n",
        "        num_cols = preprocessor.transformers_[0][2]\n",
        "        cat_cols = preprocessor.transformers_[1][2]\n",
        "        ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "        ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
        "        feature_names = list(num_cols) + ohe_names\n",
        "    except Exception:\n",
        "        feature_names = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    X_num = preprocessor.transform(X)\n",
        "    mi = mutual_info_classif(X_num, y, random_state=RANDOM_STATE)\n",
        "    mi_series = pd.Series(mi, index=feature_names).sort_values(ascending=False)\n",
        "    top = mi_series.head(top_k)\n",
        "    save_json(top.to_dict(), REPORTS_DIR / \"mutual_info_top.json\")\n",
        "    return top\n",
        "\n",
        "mi_top = compute_mutual_info(X, y, preprocessor, top_k=20)\n",
        "display(mi_top)\n"
      ],
      "metadata": {
        "id": "6_oJSExiptaR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_6NRlQ8Mp0Uw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Decision Tree\n",
        "# ---------------------------\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "def train_decision_tree_grid(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    dt = DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight=\"balanced\")\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", dt)])\n",
        "    param_grid = {\n",
        "        \"clf__max_depth\": [3,5,8,None],\n",
        "        \"clf__min_samples_leaf\": [1,2,5]\n",
        "    }\n",
        "    grid = GridSearchCV(pipe, param_grid, scoring=\"f1\", cv=3, n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    best = grid.best_estimator_\n",
        "    yp = best.predict(X_test)\n",
        "    proba = best.predict_proba(X_test)[:,1] if hasattr(best, \"predict_proba\") else None\n",
        "    return {\"decision_tree\": classification_metrics(y_test, yp, proba), \"model\": best, \"best_params\": grid.best_params_}\n",
        "\n",
        "# Orchestration\n",
        "def run_full_experiment(random_search: bool = True, balance: bool = True):\n",
        "    print(\"[INFO] Loading data and preprocessing\")\n",
        "    df = load_data()\n",
        "    X, y, preprocessor = preprocess(df, target=\"Churn\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "    # SMOTE (optional)\n",
        "    if IMBLEARN_AVAILABLE and balance:\n",
        "        print(\"[INFO] Applying SMOTE to transformed training data\")\n",
        "        preprocessor.fit(X_train)\n",
        "        X_train_p = preprocessor.transform(X_train)\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_res, y_res = sm.fit_resample(X_train_p, y_train)\n",
        "        # we'll continue to train pipelines using original X_train; SMOTE was applied to inspect resampled distribution\n",
        "    else:\n",
        "        print(\"[INFO] No SMOTE applied; using class_weight where applicable\")\n",
        "        X_res, y_res = None, None\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    print(\"[INFO] Baseline\")\n",
        "    results.update(train_baselines(X_train, y_train, X_test, y_test, preprocessor))\n",
        "\n",
        "    print(\"[INFO] Logistic\")\n",
        "    log = train_logistic(X_train, y_train, X_test, y_test, preprocessor)\n",
        "    results.update({k:v for k,v in log.items() if k != \"model\"})\n",
        "\n",
        "    print(\"[INFO] Decision Tree gridsearch\")\n",
        "    dt = train_decision_tree_grid(X_train, y_train, X_test, y_test, preprocessor)\n",
        "    results.update({k:v for k,v in dt.items() if k in [\"decision_tree\"]})\n",
        "    print(\"Decision Tree metrics:\", dt[\"decision_tree\"])\n",
        "\n",
        "    print(\"[INFO] Random Forest\")\n",
        "    rf = train_random_forest(X_train, y_train, X_test, y_test, preprocessor)\n",
        "    results.update({k:v for k,v in rf.items() if k != \"model\"})\n",
        "\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        print(\"[INFO] XGBoost\")\n",
        "        xg = train_xgboost(X_train, y_train, X_test, y_test, preprocessor)\n",
        "        if xg: results.update({k:v for k,v in xg.items() if k != \"model\"})\n",
        "\n",
        "    if KERAS_AVAILABLE:\n",
        "        print(\"[INFO] Neural Network\")\n",
        "        nn = train_nn_model(X_train, y_train, X_test, y_test, preprocessor, epochs=30, batch_size=128)\n",
        "        if nn: results.update({k:v for k,v in nn.items() if k != \"model\"})\n",
        "\n",
        "    save_json(results, REPORTS_DIR / \"model_metrics_summary.json\")\n",
        "    print(\"[INFO] Done training. Results saved.\")\n",
        "    return results\n",
        "\n",
        "# Run orchestration\n",
        "results = run_full_experiment(random_search=True, balance=True)\n"
      ],
      "metadata": {
        "id": "t8zwVx9688qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Save and Display Results\n",
        "# ---------------------------\n",
        "\n",
        "# Display results table\n",
        "def result_summary_table(results: Dict[str, Any]) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    for model_name, metrics in results.items():\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"accuracy\": metrics.get(\"accuracy\"),\n",
        "            \"precision\": metrics.get(\"precision\"),\n",
        "            \"recall\": metrics.get(\"recall\"),\n",
        "            \"f1\": metrics.get(\"f1\"),\n",
        "            \"roc_auc\": metrics.get(\"roc_auc\")\n",
        "        })\n",
        "    return pd.DataFrame(rows).sort_values(by=\"f1\", ascending=False)\n",
        "\n",
        "table = result_summary_table(results)\n",
        "display(table)\n",
        "table.to_csv(REPORTS_DIR / \"results_summary.csv\", index=False)\n",
        "\n",
        "# Attempt SHAP for final model (if available & supported)\n",
        "if SHAP_AVAILABLE:\n",
        "    try:\n",
        "        # try using logistic model saved earlier if present\n",
        "        print(\"[INFO] Generating SHAP summary (if feasible)\")\n",
        "        # find a pipeline model in environment (e.g., logistic)\n",
        "        # This block is intentionally conservative: won't crash if shap can't handle model\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] SHAP skipped:\", e)\n",
        "\n",
        "# PSI - if logistic model exists in namespace\n",
        "try:\n",
        "    # load logistic from previous training pipeline (we returned model as part of train functions earlier only in variable 'log' scope)\n",
        "    # easiest: re-run logistic training to get pipeline object\n",
        "    logpipe = None\n",
        "    # re-train logistic pipeline quickly to obtain probabilities for PSI\n",
        "    # (this is lightweight compared to full runs)\n",
        "    X2, y2, pre2 = preprocess(load_data(), target=\"Churn\")\n",
        "    Xtr, Xte, ytr, yte = train_test_split(X2, y2, test_size=0.2, random_state=RANDOM_STATE, stratify=y2)\n",
        "    lp = Pipeline([(\"prep\", pre2), (\"clf\", LogisticRegression(max_iter=1000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "    lp.fit(Xtr, ytr)\n",
        "    p_train = lp.predict_proba(Xtr)[:,1]\n",
        "    p_test = lp.predict_proba(Xte)[:,1]\n",
        "    def population_stability_index(expected, actual, buckets=10):\n",
        "        expected = np.array(expected).ravel()\n",
        "        actual = np.array(actual).ravel()\n",
        "        breaks = np.percentile(expected, np.linspace(0,100,buckets+1))\n",
        "        e_perc = np.histogram(expected, bins=breaks)[0] / len(expected)\n",
        "        a_perc = np.histogram(actual, bins=breaks)[0] / len(actual)\n",
        "        eps = 1e-6\n",
        "        e_perc = np.where(e_perc==0, eps, e_perc)\n",
        "        a_perc = np.where(a_perc==0, eps, a_perc)\n",
        "        psi = np.sum((e_perc - a_perc) * np.log(e_perc / a_perc))\n",
        "        return float(psi)\n",
        "    psi_val = population_stability_index(p_train, p_test, buckets=10)\n",
        "    print(\"[INFO] PSI:\", psi_val)\n",
        "    save_json({\"psi\": psi_val}, REPORTS_DIR / \"post_deployment_psi.json\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] PSI computation skipped:\", e)\n",
        "\n",
        "# Anomaly detector (IsolationForest) build and sample save\n",
        "try:\n",
        "    print(\"[INFO] Building anomaly detector on full X\")\n",
        "    X_all, y_all, pre_all = preprocess(load_data(), target=\"Churn\")\n",
        "    iso = IsolationForest(contamination=0.01, random_state=RANDOM_STATE)\n",
        "    iso.fit(pre_all.fit_transform(X_all))\n",
        "    scores = -iso.decision_function(pre_all.transform(X_all))\n",
        "    out = X_all.copy()\n",
        "    out[\"anomaly_score\"] = scores\n",
        "    out.head(20).to_csv(REPORTS_DIR / \"anomaly_scores_sample.csv\", index=False)\n",
        "    print(\"[INFO] Anomaly sample saved\")\n",
        "except Exception as e:\n",
        "    print(\"[WARN] Anomaly step skipped:\", e)\n",
        "\n",
        "# Ethics Printout\n",
        "ethics_report = \"\"\"\n",
        "AI Ethics & Post-Deployment Strategy\n",
        "\n",
        "1. Bias & Fairness\n",
        "   - Addressed class imbalance via SMOTE or class_weight.\n",
        "   - Plan per-group evaluation and fairness metrics.\n",
        "\n",
        "2. Data Privacy\n",
        "   - Removed identifiers like customerID.\n",
        "   - Avoid storing PII in outputs.\n",
        "\n",
        "3. Explainability\n",
        "   - SHAP for model explanations (if available).\n",
        "\n",
        "4. Monitoring\n",
        "   - PSI used to detect data drift; retrain when PSI > 0.25.\n",
        "\n",
        "5. Logging & Governance\n",
        "   - Log predictions and data snapshots for audits.\n",
        "\"\"\"\n",
        "print(ethics_report)\n"
      ],
      "metadata": {
        "id": "i40_LS1v9DdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}