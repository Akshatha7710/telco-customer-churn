{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwIKzCfCiNXh2FULjCLBqA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akshatha7710/telco-customer-churn/blob/main/telco_customer_churn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MZh7DS0FkwvL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import joblib\n",
        "import math\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score,\n",
        "    StratifiedKFold\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
        "    average_precision_score, confusion_matrix, classification_report\n",
        ")\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.ensemble import RandomForestClassifier, IsolationForest\n",
        "from sklearn.tree import DecisionTreeClassifier, export_text\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.dummy import DummyClassifier\n",
        "\n",
        "# Optional advanced models\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except Exception:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    import lightgbm as lgb\n",
        "    LIGHTGBM_AVAILABLE = True\n",
        "except Exception:\n",
        "    LIGHTGBM_AVAILABLE = False\n",
        "\n",
        "# Neural network\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "    KERAS_AVAILABLE = True\n",
        "except Exception:\n",
        "    KERAS_AVAILABLE = False\n",
        "\n",
        "# Explainability\n",
        "try:\n",
        "    import shap\n",
        "    SHAP_AVAILABLE = True\n",
        "except Exception:\n",
        "    SHAP_AVAILABLE = False\n",
        "\n",
        "try:\n",
        "    from lime import lime_tabular\n",
        "    LIME_AVAILABLE = True\n",
        "except Exception:\n",
        "    LIME_AVAILABLE = False\n",
        "\n",
        "# Imbalance handling\n",
        "try:\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    IMBLEARN_AVAILABLE = True\n",
        "except Exception:\n",
        "    IMBLEARN_AVAILABLE = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROOT = Path.cwd()\n",
        "DATA_DIR = ROOT / \"data\"\n",
        "OUT_DIR = ROOT / \"outputs\"\n",
        "FIG_DIR = OUT_DIR / \"figures\"\n",
        "MODEL_DIR = OUT_DIR / \"models\"\n",
        "REPORTS_DIR = OUT_DIR / \"reports\"\n",
        "\n",
        "for d in (DATA_DIR, OUT_DIR, FIG_DIR, MODEL_DIR, REPORTS_DIR):\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "DATA_FILE = DATA_DIR / \"Telco-Customer-Churn.csv\"\n",
        "RANDOM_STATE = 42"
      ],
      "metadata": {
        "id": "Ib-whB0Norl-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Utility Functions\n",
        "# ---------------------------\n",
        "def save_json(obj: Any, path: Path):\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(obj, f, indent=2)\n",
        "\n",
        "def ensure_df_has_target(df: pd.DataFrame, target: str = \"Churn\") -> pd.DataFrame:\n",
        "    if target not in df.columns:\n",
        "        raise ValueError(f\"target column '{target}' not in dataframe\")\n",
        "    return df\n",
        "\n",
        "def safe_head(df: pd.DataFrame, n=5):\n",
        "    print(df.head(n).to_string())"
      ],
      "metadata": {
        "id": "T_M6NL5hofO0"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Data Loading / Synthetic\n",
        "# ---------------------------\n",
        "def create_synthetic_telco(path: Path, n=2000) -> pd.DataFrame:\n",
        "    rng = np.random.default_rng(RANDOM_STATE)\n",
        "    df = pd.DataFrame({\n",
        "        \"customerID\": [f\"CUST{100000+i}\" for i in range(n)],\n",
        "        \"gender\": rng.choice([\"Female\", \"Male\"], n),\n",
        "        \"SeniorCitizen\": rng.choice([0,1], n, p=[0.85,0.15]),\n",
        "        \"Partner\": rng.choice([\"Yes\",\"No\"], n, p=[0.45,0.55]),\n",
        "        \"Dependents\": rng.choice([\"Yes\",\"No\"], n, p=[0.25,0.75]),\n",
        "        \"tenure\": rng.integers(0, 72, n),\n",
        "        \"PhoneService\": rng.choice([\"Yes\",\"No\"], n, p=[0.9,0.1]),\n",
        "        \"MultipleLines\": rng.choice([\"Yes\",\"No\",\"No phone service\"], n),\n",
        "        \"InternetService\": rng.choice([\"DSL\",\"Fiber optic\",\"No\"], n, p=[0.4,0.45,0.15]),\n",
        "        \"OnlineSecurity\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"OnlineBackup\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"DeviceProtection\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"TechSupport\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"StreamingTV\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"StreamingMovies\": rng.choice([\"Yes\",\"No\",\"No internet service\"], n),\n",
        "        \"Contract\": rng.choice([\"Month-to-month\", \"One year\", \"Two year\"], n, p=[0.6,0.2,0.2]),\n",
        "        \"PaperlessBilling\": rng.choice([\"Yes\",\"No\"], n),\n",
        "        \"PaymentMethod\": rng.choice([\n",
        "            \"Electronic check\",\"Mailed check\",\"Bank transfer (automatic)\",\"Credit card (automatic)\"\n",
        "        ], n),\n",
        "        \"MonthlyCharges\": np.round(rng.uniform(18.0, 120.0, n), 2),\n",
        "    })\n",
        "    df[\"TotalCharges\"] = np.round(df[\"MonthlyCharges\"] * df[\"tenure\"] + rng.uniform(0, 50, n), 2)\n",
        "    # churn probability function\n",
        "    churn_prob = (\n",
        "        0.35 - 0.004 * df[\"tenure\"] +\n",
        "        np.where(df[\"Contract\"] == \"Month-to-month\", 0.2, -0.05) +\n",
        "        np.where(df[\"PaymentMethod\"] == \"Electronic check\", 0.05, 0)\n",
        "    )\n",
        "    churn_prob = np.clip(churn_prob, 0.01, 0.9)\n",
        "    df[\"Churn\"] = np.where(rng.random(n) < churn_prob, \"Yes\", \"No\")\n",
        "    df.to_csv(path, index=False)\n",
        "    return df\n",
        "\n",
        "def load_data(path: Path = DATA_FILE) -> pd.DataFrame:\n",
        "    if path.exists():\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"[INFO] Loaded dataset from {path} shape={df.shape}\")\n",
        "    else:\n",
        "        print(f\"[WARN] Dataset not found at {path}. Creating synthetic dataset for development.\")\n",
        "        df = create_synthetic_telco(path)\n",
        "        print(f\"[INFO] Synthetic dataset created at {path} shape={df.shape}\")\n",
        "    # normalize column names\n",
        "    df.columns = [c.strip() for c in df.columns]\n",
        "    return df"
      ],
      "metadata": {
        "id": "vpoqnj27pC7c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Exploratory Data Analysis\n",
        "# ---------------------------\n",
        "def eda_summary(df: pd.DataFrame, target: str = \"Churn\") -> Dict[str, Any]:\n",
        "    df = df.copy()\n",
        "    summary = {}\n",
        "    summary[\"shape\"] = df.shape\n",
        "    summary[\"columns\"] = df.columns.tolist()\n",
        "    summary[\"dtypes\"] = df.dtypes.apply(lambda x: str(x)).to_dict()\n",
        "    summary[\"missing\"] = df.isnull().sum().to_dict()\n",
        "    if target in df.columns:\n",
        "        summary[\"class_distribution\"] = df[target].value_counts().to_dict()\n",
        "    # numeric stats\n",
        "    summary[\"numeric\"] = df.select_dtypes(include=[np.number]).describe().to_dict()\n",
        "    # correlation\n",
        "    try:\n",
        "        corr = df.select_dtypes(include=[np.number]).corr()\n",
        "        summary[\"correlation\"] = corr.fillna(0).to_dict()\n",
        "    except Exception:\n",
        "        summary[\"correlation\"] = {}\n",
        "    # save summary\n",
        "    save_json(summary, REPORTS_DIR / \"eda_summary.json\")\n",
        "    return summary\n",
        "\n",
        "def eda_plots(df: pd.DataFrame, target: str = \"Churn\"):\n",
        "    # churn distribution\n",
        "    if target in df.columns:\n",
        "        plt.figure(figsize=(6,4))\n",
        "        df[target].value_counts().plot(kind=\"bar\")\n",
        "        plt.title(\"Churn distribution\")\n",
        "        plt.xticks(rotation=0)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIG_DIR / \"churn_distribution.png\")\n",
        "        plt.close()\n",
        "\n",
        "    # Numeric histograms for selected columns\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    for c in [\"tenure\", \"MonthlyCharges\", \"TotalCharges\"]:\n",
        "        if c in df.columns:\n",
        "            plt.figure(figsize=(6,4))\n",
        "            df[c].dropna().hist(bins=40)\n",
        "            plt.title(f\"{c} distribution\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / f\"{c}_hist.png\")\n",
        "            plt.close()\n",
        "\n",
        "    # Correlation heatmap (numeric)\n",
        "    try:\n",
        "        import seaborn as sns\n",
        "        num = df.select_dtypes(include=[np.number])\n",
        "        if num.shape[1] > 1:\n",
        "            plt.figure(figsize=(10,8))\n",
        "            sns.heatmap(num.corr(), annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(FIG_DIR / \"numeric_corr_heatmap.png\")\n",
        "            plt.close()\n",
        "    except Exception:\n",
        "        pass"
      ],
      "metadata": {
        "id": "MqB4fV4_pYMp"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Preprocessing & Feature Engineering\n",
        "# ---------------------------\n",
        "def preprocess(df: pd.DataFrame, target: str = \"Churn\") -> Tuple[pd.DataFrame, np.ndarray, ColumnTransformer]:\n",
        "    df = df.copy()\n",
        "\n",
        "    # Remove ID column if present\n",
        "    if \"customerID\" in df.columns:\n",
        "        df = df.drop(columns=[\"customerID\"])\n",
        "\n",
        "    # Convert TotalCharges to numeric\n",
        "    if \"TotalCharges\" in df.columns:\n",
        "        df[\"TotalCharges\"] = pd.to_numeric(df[\"TotalCharges\"], errors=\"coerce\")\n",
        "\n",
        "    # Remove rows with missing target\n",
        "    df = df.dropna(subset=[target])\n",
        "\n",
        "    # Identify numeric and categoricals\n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
        "\n",
        "    # Target should not be part of features\n",
        "    if target in categorical_cols:\n",
        "        categorical_cols.remove(target)\n",
        "    if target in numeric_cols:\n",
        "        numeric_cols.remove(target)\n",
        "\n",
        "    # ---------------------------\n",
        "    # Simple feature engineering\n",
        "    # ---------------------------\n",
        "    if \"tenure\" in df.columns:\n",
        "        df[\"tenure_bin\"] = pd.cut(\n",
        "            df[\"tenure\"].fillna(-1),\n",
        "            bins=[-1, 0, 12, 24, 48, 72],\n",
        "            labels=[\"0\", \"1-12\", \"13-24\", \"25-48\", \"49-72\"]\n",
        "        )\n",
        "        categorical_cols.append(\"tenure_bin\")\n",
        "\n",
        "    # ------------------------------------------------\n",
        "    # FIX: Handle missing values in categorical columns\n",
        "    # ------------------------------------------------\n",
        "    for col in categorical_cols:\n",
        "        # If dtype is categorical, add \"Missing\" as a category\n",
        "        if df[col].dtype.name == \"category\":\n",
        "            if \"Missing\" not in df[col].cat.categories:\n",
        "                df[col] = df[col].cat.add_categories([\"Missing\"])\n",
        "\n",
        "        # Fill missing categorical values\n",
        "        df[col] = df[col].fillna(\"Missing\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # PREPROCESSING PIPELINES\n",
        "    # ---------------------------\n",
        "\n",
        "    # Numeric transformer\n",
        "    numeric_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Categorical transformer\n",
        "    categorical_transformer = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    # ColumnTransformer\n",
        "    preprocessor = ColumnTransformer([\n",
        "        (\"num\", numeric_transformer, numeric_cols),\n",
        "        (\"cat\", categorical_transformer, categorical_cols)\n",
        "    ], remainder=\"drop\")\n",
        "\n",
        "    # Encode target to binary values\n",
        "    y = df[target].apply(\n",
        "        lambda x: 1 if str(x).strip().lower() in [\"yes\", \"1\", \"true\", \"y\"] else 0\n",
        "    ).values\n",
        "\n",
        "    # X = features\n",
        "    X = df.drop(columns=[target]).copy()\n",
        "\n",
        "    # Save metadata\n",
        "    meta = {\n",
        "        \"numeric_cols\": numeric_cols,\n",
        "        \"categorical_cols\": categorical_cols,\n",
        "        \"final_shape_before_transform\": X.shape\n",
        "    }\n",
        "    save_json(meta, REPORTS_DIR / \"preprocessing_meta.json\")\n",
        "\n",
        "    return X, y, preprocessor"
      ],
      "metadata": {
        "id": "JXfL3xUQplKs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Feature selection helpers\n",
        "# ---------------------------\n",
        "def compute_mutual_info(X: pd.DataFrame, y: np.ndarray, preprocessor: ColumnTransformer, top_k: int = 20) -> List[Tuple[str,float]]:\n",
        "    # Apply preprocessor to get numeric features (but OneHotEncoder will expand features; we extract names)\n",
        "    # Fit preprocessor on X to allow get_feature_names_out (sklearn >=1.0)\n",
        "    preprocessor.fit(X)\n",
        "    try:\n",
        "        # Build feature names\n",
        "        num_cols = []\n",
        "        cat_cols = []\n",
        "        for name, trans, cols in preprocessor.transformers_:\n",
        "            if name == \"num\":\n",
        "                num_cols = cols\n",
        "            if name == \"cat\":\n",
        "                cat_cols = cols\n",
        "        # get transformed feature count & names\n",
        "        ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "        ohe_names = list(ohe.get_feature_names_out(cat_cols))\n",
        "        feature_names = list(num_cols) + ohe_names\n",
        "    except Exception:\n",
        "        # fallback: use numeric columns only\n",
        "        feature_names = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    # Transform X to numeric matrix for mutual info\n",
        "    X_num = preprocessor.transform(X)\n",
        "    mi = mutual_info_classif(X_num, y, random_state=RANDOM_STATE)\n",
        "    mi_series = pd.Series(mi, index=feature_names).sort_values(ascending=False)\n",
        "    top = list(mi_series.head(top_k).items())\n",
        "    # save\n",
        "    save_json({k: float(v) for k,v in top}, REPORTS_DIR / \"mutual_info_top.json\")\n",
        "    return top"
      ],
      "metadata": {
        "id": "6_oJSExiptaR"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Train / Evaluate utilities\n",
        "# ---------------------------\n",
        "def classification_metrics(y_true: np.ndarray, y_pred: np.ndarray, y_proba: np.ndarray = None) -> Dict[str, Any]:\n",
        "    metrics = {}\n",
        "    metrics[\"accuracy\"] = float(accuracy_score(y_true, y_pred))\n",
        "    metrics[\"precision\"] = float(precision_score(y_true, y_pred, zero_division=0))\n",
        "    metrics[\"recall\"] = float(recall_score(y_true, y_pred, zero_division=0))\n",
        "    metrics[\"f1\"] = float(f1_score(y_true, y_pred, zero_division=0))\n",
        "    if y_proba is not None:\n",
        "        try:\n",
        "            metrics[\"roc_auc\"] = float(roc_auc_score(y_true, y_proba))\n",
        "        except Exception:\n",
        "            metrics[\"roc_auc\"] = None\n",
        "        try:\n",
        "            metrics[\"pr_auc\"] = float(average_precision_score(y_true, y_proba))\n",
        "        except Exception:\n",
        "            metrics[\"pr_auc\"] = None\n",
        "    else:\n",
        "        metrics[\"roc_auc\"] = None\n",
        "        metrics[\"pr_auc\"] = None\n",
        "    metrics[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred).tolist()\n",
        "    return metrics\n",
        "\n",
        "def plot_roc_pr(y_test: np.ndarray, y_proba: np.ndarray, prefix: str):\n",
        "    # ROC\n",
        "    try:\n",
        "        from sklearn.metrics import roc_curve, precision_recall_curve, auc\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(fpr, tpr, label=f\"AUC={auc(fpr,tpr):.3f}\")\n",
        "        plt.plot([0,1],[0,1],\"--\", color=\"gray\")\n",
        "        plt.xlabel(\"FPR\")\n",
        "        plt.ylabel(\"TPR\")\n",
        "        plt.title(f\"ROC Curve - {prefix}\")\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIG_DIR / f\"{prefix}_roc.png\")\n",
        "        plt.close()\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
        "        plt.figure(figsize=(6,4))\n",
        "        plt.plot(recall, precision)\n",
        "        plt.xlabel(\"Recall\")\n",
        "        plt.ylabel(\"Precision\")\n",
        "        plt.title(f\"Precision-Recall Curve - {prefix}\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(FIG_DIR / f\"{prefix}_pr.png\")\n",
        "        plt.close()\n",
        "    except Exception:\n",
        "        pass"
      ],
      "metadata": {
        "id": "_6NRlQ8Mp0Uw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Modeling: training wrappers\n",
        "# ---------------------------\n",
        "def train_baselines(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    # Dummy baseline\n",
        "    dummy_pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", DummyClassifier(strategy=\"most_frequent\"))])\n",
        "    dummy_pipe.fit(X_train, y_train)\n",
        "    y_pred = dummy_pipe.predict(X_test)\n",
        "    y_proba = dummy_pipe.predict_proba(X_test)[:,1] if hasattr(dummy_pipe, \"predict_proba\") else None\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(dummy_pipe, MODEL_DIR / \"baseline_dummy.joblib\")\n",
        "    return {\"dummy\": metrics}\n",
        "\n",
        "def train_logistic(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))])\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    y_proba = pipe.predict_proba(X_test)[:,1]\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(pipe, MODEL_DIR / \"logistic_pipeline.joblib\")\n",
        "    return {\"logistic\": metrics, \"model\": pipe}\n",
        "\n",
        "def train_random_forest(X_train, y_train, X_test, y_test, preprocessor, random_search=False):\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1))])\n",
        "    if random_search:\n",
        "        param_dist = {\n",
        "            \"clf__n_estimators\": [100, 250, 500],\n",
        "            \"clf__max_depth\": [6, 10, 20, None],\n",
        "            \"clf__min_samples_split\": [2,5,10],\n",
        "            \"clf__max_features\": [\"sqrt\", \"log2\", 0.2, 0.5]\n",
        "        }\n",
        "        rs = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=20, scoring=\"f1\", cv=3, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "        rs.fit(X_train, y_train)\n",
        "        best = rs.best_estimator_\n",
        "    else:\n",
        "        pipe.fit(X_train, y_train)\n",
        "        best = pipe\n",
        "    y_pred = best.predict(X_test)\n",
        "    y_proba = best.predict_proba(X_test)[:,1]\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(best, MODEL_DIR / \"random_forest.joblib\")\n",
        "    return {\"random_forest\": metrics, \"model\": best}\n",
        "\n",
        "def train_xgboost(X_train, y_train, X_test, y_test, preprocessor, random_search=False):\n",
        "    if not XGBOOST_AVAILABLE:\n",
        "        print(\"[WARN] XGBoost not available; skip.\")\n",
        "        return None\n",
        "    # use sklearn wrapper\n",
        "    clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", clf)])\n",
        "    if random_search:\n",
        "        param_dist = {\n",
        "            \"clf__n_estimators\": [100, 250, 500],\n",
        "            \"clf__max_depth\": [3,6,10],\n",
        "            \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
        "            \"clf__subsample\": [0.6, 0.8, 1.0]\n",
        "        }\n",
        "        rs = RandomizedSearchCV(pipe, param_distributions=param_dist, n_iter=20, scoring=\"f1\", cv=3, n_jobs=-1, random_state=RANDOM_STATE)\n",
        "        rs.fit(X_train, y_train)\n",
        "        best = rs.best_estimator_\n",
        "    else:\n",
        "        pipe.fit(X_train, y_train)\n",
        "        best = pipe\n",
        "    y_pred = best.predict(X_test)\n",
        "    y_proba = best.predict_proba(X_test)[:,1]\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "    joblib.dump(best, MODEL_DIR / \"xgboost.joblib\")\n",
        "    return {\"xgboost\": metrics, \"model\": best}\n",
        "\n",
        "def train_hist_gbm(X_train, y_train, X_test, y_test, preprocessor):\n",
        "    # fallback to sklearn's HistGradientBoostingClassifier\n",
        "    try:\n",
        "        from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "        clf = HistGradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "        pipe = Pipeline([(\"prep\", preprocessor), (\"clf\", clf)])\n",
        "        pipe.fit(X_train, y_train)\n",
        "        y_pred = pipe.predict(X_test)\n",
        "        # HGB does not always have predict_proba for some versions; handle gracefully\n",
        "        y_proba = pipe.predict_proba(X_test)[:,1] if hasattr(pipe, \"predict_proba\") else None\n",
        "        metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "        joblib.dump(pipe, MODEL_DIR / \"hist_gbm.joblib\")\n",
        "        return {\"hist_gbm\": metrics, \"model\": pipe}\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] HistGradientBoosting unavailable:\", e)\n",
        "        return None\n",
        "\n",
        "def train_nn_model(X_train, y_train, X_test, y_test, preprocessor, epochs=30, batch_size=64):\n",
        "    if not KERAS_AVAILABLE:\n",
        "        print(\"[WARN] Keras/TensorFlow not available; skipping NN.\")\n",
        "        return None\n",
        "    # Preprocess to dense numpy\n",
        "    X_train_p = preprocessor.fit_transform(X_train)\n",
        "    X_test_p = preprocessor.transform(X_test)\n",
        "    input_dim = X_train_p.shape[1]\n",
        "\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(256, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dropout(0.4),\n",
        "        layers.Dense(128, activation=\"relu\"),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(1, activation=\"sigmoid\")\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"binary_crossentropy\", metrics=[keras.metrics.AUC(name=\"auc\")])\n",
        "    early_stop = keras.callbacks.EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=6, restore_best_weights=True)\n",
        "    history = model.fit(X_train_p, y_train, validation_split=0.1, epochs=epochs, batch_size=batch_size, callbacks=[early_stop], verbose=0)\n",
        "\n",
        "    y_proba = model.predict(X_test_p).ravel()\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "    metrics = classification_metrics(y_test, y_pred, y_proba)\n",
        "\n",
        "    # Save model and preprocessor\n",
        "    model_path = MODEL_DIR / \"nn_model.keras\"\n",
        "    model.save(model_path)\n",
        "    joblib.dump(preprocessor, MODEL_DIR / \"nn_preprocessor.joblib\")\n",
        "    # Save history\n",
        "    hist_path = REPORTS_DIR / \"nn_history.json\"\n",
        "    save_json({k: [float(v) for v in vals] for k, vals in history.history.items()}, hist_path)\n",
        "\n",
        "    return {\"neural_network\": metrics, \"model\": (model, preprocessor)}\n"
      ],
      "metadata": {
        "id": "q2BG6Mz_p67x"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Explainability (SHAP / LIME)\n",
        "# ---------------------------\n",
        "def explain_with_shap(model_pipe, X_train: pd.DataFrame, sample_X_test: pd.DataFrame, model_name: str):\n",
        "    if not SHAP_AVAILABLE:\n",
        "        print(\"[WARN] SHAP not installed; install shap to get explanations.\")\n",
        "        return None\n",
        "\n",
        "    print(f\"[INFO] Generating SHAP explanations for {model_name}...\")\n",
        "\n",
        "    # extract the underlying model and preprocessor\n",
        "    if isinstance(model_pipe, Pipeline):\n",
        "        pre = model_pipe.named_steps.get(\"prep\")\n",
        "        clf = model_pipe.named_steps.get(\"clf\")\n",
        "    else:\n",
        "        # for (model, preprocessor) tuple\n",
        "        try:\n",
        "            clf, pre = model_pipe\n",
        "        except Exception:\n",
        "            print(\"[WARN] Unknown model_pipe structure for SHAP\")\n",
        "            return None\n",
        "\n",
        "    X_sample_trans = pre.transform(sample_X_test)\n",
        "    # Use TreeExplainer for tree-based models\n",
        "    try:\n",
        "        explainer = shap.Explainer(clf)\n",
        "        shap_values = explainer(X_sample_trans)\n",
        "    except Exception:\n",
        "        # fallback: KernelExplainer (slower)\n",
        "        explainer = shap.KernelExplainer(lambda x: clf.predict_proba(x)[:,1], shap.sample(X_sample_trans, 100))\n",
        "        shap_values = explainer.shap_values(X_sample_trans[:100])\n",
        "    # Save a small summary plot\n",
        "    try:\n",
        "        shap.summary_plot(shap_values, X_sample_trans, show=False)\n",
        "        plt.savefig(FIG_DIR / f\"shap_summary_{model_name}.png\", bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "    except Exception:\n",
        "        pass\n",
        "    return shap_values\n",
        "\n",
        "def explain_with_lime(model_pipe, X_train: pd.DataFrame, sample_row: pd.Series):\n",
        "    if not LIME_AVAILABLE:\n",
        "        print(\"[WARN] LIME not installed; install lime to get local explanations.\")\n",
        "        return None\n",
        "    # LIME needs numpy arrays and feature names\n",
        "    pre = model_pipe.named_steps.get(\"prep\")\n",
        "    clf = model_pipe.named_steps.get(\"clf\")\n",
        "    X_train_p = pre.transform(X_train)\n",
        "    feature_names = []\n",
        "    # try to get OHE feature names\n",
        "    try:\n",
        "        cat_step = pre.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
        "        cat_cols = pre.transformers_[1][2]\n",
        "        ohe_names = list(cat_step.get_feature_names_out(cat_cols))\n",
        "        num_cols = pre.transformers_[0][2]\n",
        "        feature_names = list(num_cols) + ohe_names\n",
        "    except Exception:\n",
        "        # fallback\n",
        "        feature_names = [f\"f{i}\" for i in range(X_train_p.shape[1])]\n",
        "    explainer = lime_tabular.LimeTabularExplainer(X_train_p, feature_names=feature_names, class_names=[\"No\",\"Yes\"], discretize_continuous=True)\n",
        "    row_p = pre.transform(pd.DataFrame([sample_row]))\n",
        "    exp = explainer.explain_instance(row_p[0], clf.predict_proba, num_features=10)\n",
        "    return exp"
      ],
      "metadata": {
        "id": "vSnnyHo-qBG-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Post-deployment: Drift detection (PSI)\n",
        "# ---------------------------\n",
        "def population_stability_index(expected: np.ndarray, actual: np.ndarray, buckets: int = 10) -> float:\n",
        "    \"\"\"Compute PSI between expected and actual arrays (numeric)\"\"\"\n",
        "    expected = np.array(expected).ravel()\n",
        "    actual = np.array(actual).ravel()\n",
        "    # build quantile bins from expected\n",
        "    breakpoints = np.percentile(expected, np.linspace(0, 100, buckets+1))\n",
        "    expected_perc = np.histogram(expected, bins=breakpoints)[0] / len(expected)\n",
        "    actual_perc = np.histogram(actual, bins=breakpoints)[0] / len(actual)\n",
        "    # avoid zeros\n",
        "    eps = 1e-6\n",
        "    expected_perc = np.where(expected_perc == 0, eps, expected_perc)\n",
        "    actual_perc = np.where(actual_perc == 0, eps, actual_perc)\n",
        "    psi = np.sum((expected_perc - actual_perc) * np.log(expected_perc / actual_perc))\n",
        "    return float(psi)"
      ],
      "metadata": {
        "id": "rUTP1HWfqHTd"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Anomaly detection (IsolationForest)\n",
        "# ---------------------------\n",
        "def build_anomaly_detector(X: pd.DataFrame, preprocessor: ColumnTransformer):\n",
        "    X_p = preprocessor.fit_transform(X)\n",
        "    iso = IsolationForest(contamination=0.01, random_state=RANDOM_STATE)\n",
        "    iso.fit(X_p)\n",
        "    joblib.dump(iso, MODEL_DIR / \"isolation_forest.joblib\")\n",
        "    return iso\n",
        "\n",
        "def score_anomalies(iso: IsolationForest, X: pd.DataFrame, preprocessor: ColumnTransformer):\n",
        "    X_p = preprocessor.transform(X)\n",
        "    scores = -iso.decision_function(X_p)  # higher = more anomalous\n",
        "    return scores"
      ],
      "metadata": {
        "id": "bb8al4x6qLvE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Real-time inference helper\n",
        "# ---------------------------\n",
        "def build_predict_fn(model_pipe) -> callable:\n",
        "    \"\"\"\n",
        "    Returns a function predict_fn(dict_row)->(pred, proba)\n",
        "    \"\"\"\n",
        "    def predict_fn(input_dict: Dict[str, Any]):\n",
        "        row = pd.DataFrame([input_dict])\n",
        "        if isinstance(model_pipe, Pipeline):\n",
        "            proba = model_pipe.predict_proba(row)[:,1]\n",
        "            pred = (proba >= 0.5).astype(int)\n",
        "            return int(pred[0]), float(proba[0])\n",
        "        else:\n",
        "            # tuple (model, preprocessor) expected for NN\n",
        "            try:\n",
        "                model, pre = model_pipe\n",
        "                Xp = pre.transform(row)\n",
        "                proba = model.predict(Xp).ravel()\n",
        "                pred = (proba >= 0.5).astype(int)\n",
        "                return int(pred[0]), float(proba[0])\n",
        "            except Exception as e:\n",
        "                raise RuntimeError(\"Unknown model_pipe format for prediction\") from e\n",
        "\n",
        "    return predict_fn"
      ],
      "metadata": {
        "id": "ktyRmhA7qRj-"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Orchestrator: run full experiment\n",
        "# ---------------------------\n",
        "def run_full_experiment(random_search: bool = True, balance: bool = True):\n",
        "    print(\"[INFO] Loading data\")\n",
        "    df = load_data()\n",
        "    ensure_df_has_target(df, \"Churn\")\n",
        "    eda_summary(df, \"Churn\")\n",
        "    eda_plots(df, \"Churn\")\n",
        "\n",
        "    print(\"[INFO] Preprocessing data\")\n",
        "    X, y, preprocessor = preprocess(df, target=\"Churn\")\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=RANDOM_STATE, stratify=y)\n",
        "\n",
        "    # Handle imbalance with SMOTE if available and requested\n",
        "    if IMBLEARN_AVAILABLE and balance:\n",
        "        print(\"[INFO] Applying SMOTE to training data\")\n",
        "        X_train_prep = preprocessor.fit_transform(X_train)\n",
        "        sm = SMOTE(random_state=RANDOM_STATE)\n",
        "        X_res, y_res = sm.fit_resample(X_train_prep, y_train)\n",
        "        # after resampling, we cannot easily inverse-transform to DataFrame; so for model training we will\n",
        "        # fit models on pipelines that accept raw X_train and pipeline internal preprocessor will be re-fit.\n",
        "        # To keep things consistent, we will re-fit pipeline directly on raw X_train but with class_weight if needed.\n",
        "        use_smote = True\n",
        "    else:\n",
        "        X_res, y_res = None, None\n",
        "        use_smote = False\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Baseline\n",
        "    print(\"[INFO] Training baseline models\")\n",
        "    results.update(train_baselines(X_train, y_train, X_test, y_test, preprocessor))\n",
        "\n",
        "    # Logistic\n",
        "    print(\"[INFO] Training Logistic Regression\")\n",
        "    log_res = train_logistic(X_train, y_train, X_test, y_test, preprocessor)\n",
        "    results.update({k:v for k,v in log_res.items() if k != \"model\"})\n",
        "\n",
        "    # Random Forest\n",
        "    print(\"[INFO] Training Random Forest\")\n",
        "    rf_res = train_random_forest(X_train, y_train, X_test, y_test, preprocessor, random_search=random_search)\n",
        "    results.update({k:v for k,v in rf_res.items() if k != \"model\"})\n",
        "    rf_model = rf_res.get(\"model\")\n",
        "\n",
        "    # XGBoost or HistGradientBoosting\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        print(\"[INFO] Training XGBoost\")\n",
        "        xgb_res = train_xgboost(X_train, y_train, X_test, y_test, preprocessor, random_search=random_search)\n",
        "        if xgb_res is not None:\n",
        "            results.update({k:v for k,v in xgb_res.items() if k != \"model\"})\n",
        "    else:\n",
        "        print(\"[INFO] Training HistGradientBoosting (fallback)\")\n",
        "        hgb_res = train_hist_gbm(X_train, y_train, X_test, y_test, preprocessor)\n",
        "        if hgb_res:\n",
        "            results.update({k:v for k,v in hgb_res.items() if k != \"model\"})\n",
        "\n",
        "    # Neural Network\n",
        "    print(\"[INFO] Training Neural Network (may take a while)\")\n",
        "    nn_res = train_nn_model(X_train, y_train, X_test, y_test, preprocessor, epochs=30, batch_size=128)\n",
        "    if nn_res:\n",
        "        results.update({k:v for k,v in nn_res.items() if k != \"model\"})\n",
        "\n",
        "    # Compare results and choose best by f1 or roc_auc\n",
        "    save_json(results, REPORTS_DIR / \"model_metrics_summary.json\")\n",
        "\n",
        "    # Example: use random forest pipeline as final model if available\n",
        "    try:\n",
        "        final_model = rf_model\n",
        "    except Exception:\n",
        "        final_model = None\n",
        "\n",
        "    if final_model is None:\n",
        "        print(\"[WARN] No final model found; using logistic pipeline as final fallback.\")\n",
        "        try:\n",
        "            final_model = joblib.load(MODEL_DIR / \"logistic_pipeline.joblib\")\n",
        "        except Exception:\n",
        "            final_model = None\n",
        "\n",
        "    if final_model is not None:\n",
        "        print(\"[INFO] Generating explanations for final model (SHAP if available)\")\n",
        "        # pick sample of test set rows for explainability\n",
        "        sample_X_test = X_test.sample(n=min(200, max(1, len(X_test))), random_state=RANDOM_STATE)\n",
        "        if SHAP_AVAILABLE:\n",
        "            try:\n",
        "                shap_values = explain_with_shap(final_model, X_train, sample_X_test, \"final_model\")\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] SHAP explanation failed:\", e)\n",
        "        if LIME_AVAILABLE:\n",
        "            try:\n",
        "                exp = explain_with_lime(final_model, X_train, sample_X_test.iloc[0])\n",
        "                # save lime explanation as text\n",
        "                with open(FIG_DIR / \"lime_exp.txt\", \"w\") as f:\n",
        "                    f.write(str(exp.as_list()))\n",
        "            except Exception as e:\n",
        "                print(\"[WARN] LIME explanation failed:\", e)\n",
        "\n",
        "        # Build predict function and save\n",
        "        predict_fn = build_predict_fn(final_model)\n",
        "        # Example prediction saved\n",
        "        example_row = X_test.iloc[0].to_dict()\n",
        "        pred, proba = predict_fn(example_row)\n",
        "        save_json({\"example_prediction\": {\"pred\": pred, \"proba\": proba, \"row\": example_row}}, REPORTS_DIR / \"example_prediction.json\")\n",
        "\n",
        "    # Build anomaly detector on whole training data\n",
        "    try:\n",
        "        print(\"[INFO] Building anomaly detector\")\n",
        "        iso = build_anomaly_detector(X, preprocessor)\n",
        "        scores = score_anomalies(iso, X_test, preprocessor)\n",
        "        # attach scores to sample and save\n",
        "        out_df = X_test.copy()\n",
        "        out_df[\"anomaly_score\"] = scores\n",
        "        out_df.head(20).to_csv(REPORTS_DIR / \"anomaly_scores_sample.csv\", index=False)\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] Anomaly detector failed:\", e)\n",
        "\n",
        "    # Post-deployment: compute PSI between training probability distribution and test probability distribution for a model (if computed)\n",
        "    try:\n",
        "        # use logistic model proba if available\n",
        "        try:\n",
        "            logistic_pipe = joblib.load(MODEL_DIR / \"logistic_pipeline.joblib\")\n",
        "            proba_train = logistic_pipe.predict_proba(X_train)[:,1]\n",
        "            proba_test = logistic_pipe.predict_proba(X_test)[:,1]\n",
        "            psi = population_stability_index(proba_train, proba_test, buckets=10)\n",
        "            save_json({\"psi\": psi}, REPORTS_DIR / \"post_deployment_psi.json\")\n",
        "            print(f\"[INFO] PSI computed: {psi:.5f}\")\n",
        "        except Exception:\n",
        "            pass\n",
        "    except Exception as e:\n",
        "        print(\"[WARN] PSI computation failed:\", e)\n",
        "\n",
        "    print(\"[INFO] Full experiment complete. Outputs written to:\", OUT_DIR)\n",
        "    return results"
      ],
      "metadata": {
        "id": "Q3SlyjeAqWo1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Run when executed directly\n",
        "# ---------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    start_time = time.time()\n",
        "    results = run_full_experiment(random_search=True, balance=True)\n",
        "    elapsed = time.time() - start_time\n",
        "    print(f\"[INFO] Total time elapsed: {elapsed/60:.2f} minutes\")\n",
        "    # print brief results\n",
        "    try:\n",
        "        print(json.dumps(results, indent=2))\n",
        "    except Exception:\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3nnwyneqavT",
        "outputId": "8499273b-fb31-4ce3-ca9a-07a11ace9756"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading data\n",
            "[INFO] Loaded dataset from /content/data/Telco-Customer-Churn.csv shape=(2000, 21)\n",
            "[INFO] Preprocessing data\n",
            "[INFO] Applying SMOTE to training data\n",
            "[INFO] Training baseline models\n",
            "[INFO] Training Logistic Regression\n",
            "[INFO] Training Random Forest\n",
            "[INFO] Training XGBoost\n",
            "[INFO] Training Neural Network (may take a while)\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n",
            "[INFO] Generating explanations for final model (SHAP if available)\n",
            "[INFO] Generating SHAP explanations for final_model...\n",
            "[INFO] Building anomaly detector\n",
            "[INFO] PSI computed: 0.08605\n",
            "[INFO] Full experiment complete. Outputs written to: /content/outputs\n",
            "[INFO] Total time elapsed: 1.25 minutes\n",
            "{\n",
            "  \"dummy\": {\n",
            "    \"accuracy\": 0.6925,\n",
            "    \"precision\": 0.0,\n",
            "    \"recall\": 0.0,\n",
            "    \"f1\": 0.0,\n",
            "    \"roc_auc\": 0.5,\n",
            "    \"pr_auc\": 0.3075,\n",
            "    \"confusion_matrix\": [\n",
            "      [\n",
            "        277,\n",
            "        0\n",
            "      ],\n",
            "      [\n",
            "        123,\n",
            "        0\n",
            "      ]\n",
            "    ]\n",
            "  },\n",
            "  \"logistic\": {\n",
            "    \"accuracy\": 0.685,\n",
            "    \"precision\": 0.48514851485148514,\n",
            "    \"recall\": 0.3983739837398374,\n",
            "    \"f1\": 0.4375,\n",
            "    \"roc_auc\": 0.6740923365912359,\n",
            "    \"pr_auc\": 0.4263930863156389,\n",
            "    \"confusion_matrix\": [\n",
            "      [\n",
            "        225,\n",
            "        52\n",
            "      ],\n",
            "      [\n",
            "        74,\n",
            "        49\n",
            "      ]\n",
            "    ]\n",
            "  },\n",
            "  \"random_forest\": {\n",
            "    \"accuracy\": 0.6825,\n",
            "    \"precision\": 0.4807692307692308,\n",
            "    \"recall\": 0.4065040650406504,\n",
            "    \"f1\": 0.44052863436123346,\n",
            "    \"roc_auc\": 0.6504065040650405,\n",
            "    \"pr_auc\": 0.41437311202649785,\n",
            "    \"confusion_matrix\": [\n",
            "      [\n",
            "        223,\n",
            "        54\n",
            "      ],\n",
            "      [\n",
            "        73,\n",
            "        50\n",
            "      ]\n",
            "    ]\n",
            "  },\n",
            "  \"xgboost\": {\n",
            "    \"accuracy\": 0.6575,\n",
            "    \"precision\": 0.43859649122807015,\n",
            "    \"recall\": 0.4065040650406504,\n",
            "    \"f1\": 0.4219409282700422,\n",
            "    \"roc_auc\": 0.6255466525784392,\n",
            "    \"pr_auc\": 0.3905437240242603,\n",
            "    \"confusion_matrix\": [\n",
            "      [\n",
            "        213,\n",
            "        64\n",
            "      ],\n",
            "      [\n",
            "        73,\n",
            "        50\n",
            "      ]\n",
            "    ]\n",
            "  },\n",
            "  \"neural_network\": {\n",
            "    \"accuracy\": 0.685,\n",
            "    \"precision\": 0.48484848484848486,\n",
            "    \"recall\": 0.3902439024390244,\n",
            "    \"f1\": 0.43243243243243246,\n",
            "    \"roc_auc\": 0.6601948871474275,\n",
            "    \"pr_auc\": 0.4440958228834684,\n",
            "    \"confusion_matrix\": [\n",
            "      [\n",
            "        226,\n",
            "        51\n",
            "      ],\n",
            "      [\n",
            "        75,\n",
            "        48\n",
            "      ]\n",
            "    ]\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}